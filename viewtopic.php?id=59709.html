<!DOCTYPE html>
<html lang="en-US">
<head>

	<title>OpenWrt Forum Archive</title>

	<meta charset="UTF-8">

	<meta http-equiv="X-UA-Compatible" content="IE=edge">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="stylesheet" href="assets/css/common.css">

</head>
<body>

<div class="container">

<header class="main-header">
	<h1 class="logo"><a href="index.html"><img src="assets/img/logo.png" width="376" height="88" alt="OpenWrt Forum Archive"></a></h1>
</header>

<aside>
	<p>This is a read-only archive of the old OpenWrt forum. The current OpenWrt forum resides at <a href="https://forum.openwrt.org/">https://forum.openwrt.org/</a>.</p>
	<p class="minor">In May 2018, the OpenWrt forum suffered a total data loss. This archive is an effort to restore and make available as much content as possible. Content may be missing or not representing the latest edited version.</p>
</aside>

<main>
	<header>
		<h1><span class="minor">Topic:</span> SQM/QoS Help</h1>
	</header>
	<div class="notice minor">
		<p>
			The content of this topic has been archived
							on 17 Apr 2018.
										There are no obvious gaps in this topic, but there may still be some posts missing at the end.
					</p>
	</div>

	<div class="pagination"><div class="pagination-number">Page 1 of 1</div><nav><ul><li class="pagination-current"><span>1</span></li></ul></nav></div>
			
		
		
			<article class="post" id="p292282">
				<div class="post-metadata">
					<div class="post-num">Post #1</div>
					<div class="post-author">iannmckay</div>
					<div class="post-datetime">
						17 Sep 2015, 20:48					</div>
				</div>
				<div class="post-content content">
					<p>I play CS:GO on a semi-professional level and I have been experiencing consistent ping spikes. I think it is likely due to my ISP but I switched over to OpenWRT and enabled SQM to make sure it wasn&#039;t bufferbloat or anything similar. </p><p>Here&#039;s a quick example video taken last night: <a href="https://www.youtube.com/watch?v=Q9jFrmFJ3o8">https://www.youtube.com/watch?v=Q9jFrmFJ3o8</a></p><p>If you look at the bottom, you can see that my ping is 65ish, and then it will shoot up to 140 for a split second and then go back down. During this time the game gets very choppy and enemy player models warp around. </p><p>Unfortunately OpenWRT/SQM hasn&#039;t helped with the issue that much. I have roommate who sometimes uses netflix/PSN so I was wondering if there was a way to limit his bandwidth specifically and seeing if that helps?</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292336">
				<div class="post-metadata">
					<div class="post-num">Post #2</div>
					<div class="post-author">drawz</div>
					<div class="post-datetime">
						18 Sep 2015, 05:05					</div>
				</div>
				<div class="post-content content">
					<p>How have you configured it? What does an extended speedtest (not speedtest.net) show as your download &amp; upload speed? Can you try to determine if the ping spikes are really when your roommate is using the connection?</p><p>In theory, SQM (if configured correctly) should take care of this.</p>											<p class="post-edited">(Last edited by <strong>drawz</strong> on 18 Sep 2015, 05:05)</p>
									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292558">
				<div class="post-metadata">
					<div class="post-num">Post #3</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						19 Sep 2015, 11:26					</div>
				</div>
				<div class="post-content content">
					<p>Hi ianmckay,</p><div class="quotebox"><cite>iannmckay wrote:</cite><blockquote><p>I play CS:GO on a semi-professional level and I have been experiencing consistent ping spikes. I think it is likely due to my ISP but I switched over to OpenWRT and enabled SQM to make sure it wasn&#039;t bufferbloat or anything similar. </p><p>Here&#039;s a quick example video taken last night: <a href="https://www.youtube.com/watch?v=Q9jFrmFJ3o8">https://www.youtube.com/watch?v=Q9jFrmFJ3o8</a></p><p>If you look at the bottom, you can see that my ping is 65ish, and then it will shoot up to 140 for a split second and then go back down. During this time the game gets very choppy and enemy player models warp around. </p><p>Unfortunately OpenWRT/SQM hasn&#039;t helped with the issue that much. I have roommate who sometimes uses netflix/PSN so I was wondering if there was a way to limit his bandwidth specifically and seeing if that helps?</p></blockquote></div><p>This is now the second report of sqm not working well with netflix. It would be great if you could help figuring out the root cause for this. There are a number of places where one can pick up unwanted delay and sqm only has control over one of those, so the first thing we need is a repeatable test case (otherwise trying to find the root-cause is going to be glacial).</p><p>Do you think you could convince your room-mate to help in diagnosing this issue? If yes, what about you try create a test case like: you play your game and your room mate start streaming and stops streaming cyclically and you figure out whether your latency spikes are truly correlated to his streaming. I would also recommend using a tool like mtr, if possible directed to the same IP address as your game traffic, concurrently to your game. Ideally game lag and mtr will show the same latency spikes and you could avoid using the game for debugging from that point on. Note mtr will give you sort of an on-line tracerouter that might also give a hint where the delay is accrued, so if you have data, do not hesitate to post it here <img src="https://forum.openwrt.org/img/smilies/wink.png" width="15" height="15" alt="wink" /><br />Also it would be helpful, if you could describe your network topology a bit (or better in detail <img src="https://forum.openwrt.org/img/smilies/wink.png" width="15" height="15" alt="wink" /> ). So what router and/or modem are you using, which ISP are you using, are your machines connected via cables or via wlan? How are your room mates machines connected? Are there any other devices connected to your network... you get the idea. Do the spikes go away if you disable wlan completely?</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp;M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292784">
				<div class="post-metadata">
					<div class="post-num">Post #4</div>
					<div class="post-author">ngraap</div>
					<div class="post-datetime">
						21 Sep 2015, 07:39					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p>Hi ianmckay,</p><div class="quotebox"><cite>iannmckay wrote:</cite><blockquote><p>I play CS:GO on a semi-professional level and I have been experiencing consistent ping spikes. I think it is likely due to my ISP but I switched over to OpenWRT and enabled SQM to make sure it wasn&#039;t bufferbloat or anything similar. </p><p>Here&#039;s a quick example video taken last night: <a href="https://www.youtube.com/watch?v=Q9jFrmFJ3o8">https://www.youtube.com/watch?v=Q9jFrmFJ3o8</a></p><p>If you look at the bottom, you can see that my ping is 65ish, and then it will shoot up to 140 for a split second and then go back down. During this time the game gets very choppy and enemy player models warp around. </p><p>Unfortunately OpenWRT/SQM hasn&#039;t helped with the issue that much. I have roommate who sometimes uses netflix/PSN so I was wondering if there was a way to limit his bandwidth specifically and seeing if that helps?</p></blockquote></div><p>This is now the second report of sqm not working well with netflix. It would be great if you could help figuring out the root cause for this. There are a number of places where one can pick up unwanted delay and sqm only has control over one of those, so the first thing we need is a repeatable test case (otherwise trying to find the root-cause is going to be glacial).</p><p>Do you think you could convince your room-mate to help in diagnosing this issue? If yes, what about you try create a test case like: you play your game and your room mate start streaming and stops streaming cyclically and you figure out whether your latency spikes are truly correlated to his streaming. I would also recommend using a tool like mtr, if possible directed to the same IP address as your game traffic, concurrently to your game. Ideally game lag and mtr will show the same latency spikes and you could avoid using the game for debugging from that point on. Note mtr will give you sort of an on-line tracerouter that might also give a hint where the delay is accrued, so if you have data, do not hesitate to post it here <img src="https://forum.openwrt.org/img/smilies/wink.png" width="15" height="15" alt="wink" /><br />Also it would be helpful, if you could describe your network topology a bit (or better in detail <img src="https://forum.openwrt.org/img/smilies/wink.png" width="15" height="15" alt="wink" /> ). So what router and/or modem are you using, which ISP are you using, are your machines connected via cables or via wlan? How are your room mates machines connected? Are there any other devices connected to your network... you get the idea. Do the spikes go away if you disable wlan completely?</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp;M.</p></blockquote></div><p>I have two roomates both stream netflix and comcast heavily and one of them downloads a lot. I also play CSGO but never have an issue, I&#039;d say it&#039;s improved using openwrt and SQM but I&#039;m on cake so I don&#039;t know if I&#039;m of any help in this situation</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292789">
				<div class="post-metadata">
					<div class="post-num">Post #5</div>
					<div class="post-author">makarel</div>
					<div class="post-datetime">
						21 Sep 2015, 09:12					</div>
				</div>
				<div class="post-content content">
					<p>activate aimbot, norecoil/nospread optional wall - you wont feel any lag anymore <img src="https://forum.openwrt.org/img/smilies/big_smile.png" width="15" height="15" alt="big_smile" /></p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292803">
				<div class="post-metadata">
					<div class="post-num">Post #6</div>
					<div class="post-author">Magnetz</div>
					<div class="post-datetime">
						21 Sep 2015, 13:03					</div>
				</div>
				<div class="post-content content">
					<p>I&#039;ve experienced the same thing with SQM QOS package. Anything that opens many TCP conversations seems to cause it - you can view the number of TCP conversations in wireshark.</p><p>Things that helped for me were to set the interval at 50ms and to classify UDP traffic in the priority queue in the sqm script.</p><p>I&#039;ve not done extensive testing but it made a small improvement.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292805">
				<div class="post-metadata">
					<div class="post-num">Post #7</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						21 Sep 2015, 13:32					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>Magnetz wrote:</cite><blockquote><p>I&#039;ve experienced the same thing with SQM QOS package. Anything that opens many TCP conversations seems to cause it - you can view the number of TCP conversations in wireshark.</p><p>Things that helped for me were to set the interval at 50ms and to classify UDP traffic in the priority queue in the sqm script.</p><p>I&#039;ve not done extensive testing but it made a small improvement.</p></blockquote></div><p>Hi Magnetz,</p><p>TCP nowadays will inject 10 packets on connection startup (goggle initial window / iw10). If enough incoming connections are established at the same time these these can cause congestion of the incoming connection at the ISP end and hence result in buffer bloat. <br />The next possible issue is that fq_codel will try to special case new connections by treating them a bit faster than established connections with continuous traffic (this is on purpose, often sparse flows are latency sensitive and once fq_codel learns that the flow is not sparse this preferential treatment stops). </p><p>But I had thought that neflix streaming basically uses only a few (1?) connection so I am not sure the data already shows that netflix is behaving like that. If you have any insight in what happens from looking at wireshark data, please let us know.</p><p>Reducing the interval to 50ms will come at the cost of reducing the throughput of connections with longer RTTs (admittedly for north americans many connections are shorter, especially to those CDNs that might cause a packet inrush).<br />Special casing your important traffic by placing it into a higher priority band is good advice. Unfortunately also hard to do... SQM has very good control over the outgoing or egress traffic, but for incoming or ingress traffic the control is much more limited. Let me elaborate; on egress we can use DSCP markings for each packet (and configure the applications sending the packets to set those markings) to select the appropriate priority band. But for ingress we typically can not: </p><p>1) It generally is not considered a good idea to simply trust incoming DSCP markings as any hostile outside party can set their own markings, though you can configure SQM to do so. (Note in SQM we have two options for ingress DSCP marks exposed that are independent of each other: &quot;Squash DSCP on inbound packets (ingress):&quot; will re-map the DSCP bits to all zero, but only after SQM itself had taken the initial content of these bits into account. And &quot;Ignore DSCP on ingress:&quot; which uses a simple one priority-tier only shaper on ingress.) It might be interesting to play with the second option (if one uses simple.qos or layer_cake.qos) and see whether this improves behavior under stress. </p><p>2) We can not simply copy the DSCP markings from an established conn tracked flow from egress to ingress as ingress shaping happens outside of iptables if I understand correctly. If one sets up SQM not on the true WAN interface but say on an interface connecting the router to a switch/2ndary router (to wich all other machines are connected) The primary router should still be able to do connection tracking and DSCP mark copying and SQM could the trust the ingress DSCP marking. But so far this is conjecture, none of the scripts is set up to this I believe and it might not work.</p><p>3) On ingress we create an artificial bottleneck on the &quot;wrong&quot; end of the true bottleneck, which works better if the ratio between artificial bottleneck rate to true bottleneck rate gets smaller (so the more bandwidth one sacrifices the tighter the latency control). But if the packet inrush is large enough to also fill the true bottleneck buffer on the ISP&#039;s end of the link, and we have no control over this added latency. Reducing the interval might actually affect this, with a shorter interval SQM will be more trigger-happy to signal congestion back to the senders which should make ISP-side uncontrolled buffers react a bit snappier, but at the cost of more bandwidth for long-RTT connections. It would be good if the OP could test this and report back though.</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292814">
				<div class="post-metadata">
					<div class="post-num">Post #8</div>
					<div class="post-author">Magnetz</div>
					<div class="post-datetime">
						21 Sep 2015, 15:12					</div>
				</div>
				<div class="post-content content">
					<p>I don&#039;t actually use netflix but I was having issues with the dota 2 game client which would open a news/blog page every time it was started and download ~20MB over several streams. Which takes a fairly long time on 3.8Mb DSL.</p><p>They have since changed the game client and it no longer does that, but large web pages can still cause ping spikes.</p><p>With an otherwise idle connection this is what happens if I ping google.com while loading dailymail.co.uk</p><div class="codebox"><pre><code>Reply from 216.58.210.46: bytes=32 time=22ms TTL=58
Reply from 216.58.210.46: bytes=32 time=54ms TTL=58
Reply from 216.58.210.46: bytes=32 time=21ms TTL=58
Reply from 216.58.210.46: bytes=32 time=21ms TTL=58
Reply from 216.58.210.46: bytes=32 time=21ms TTL=58
Reply from 216.58.210.46: bytes=32 time=54ms TTL=58
Reply from 216.58.210.46: bytes=32 time=24ms TTL=58
Reply from 216.58.210.46: bytes=32 time=102ms TTL=58
Reply from 216.58.210.46: bytes=32 time=55ms TTL=58
Reply from 216.58.210.46: bytes=32 time=66ms TTL=58
Reply from 216.58.210.46: bytes=32 time=48ms TTL=58
Reply from 216.58.210.46: bytes=32 time=37ms TTL=58
Reply from 216.58.210.46: bytes=32 time=88ms TTL=58
Reply from 216.58.210.46: bytes=32 time=34ms TTL=58
Reply from 216.58.210.46: bytes=32 time=35ms TTL=58
Reply from 216.58.210.46: bytes=32 time=43ms TTL=58
Reply from 216.58.210.46: bytes=32 time=46ms TTL=58
Reply from 216.58.210.46: bytes=32 time=62ms TTL=58
Reply from 216.58.210.46: bytes=32 time=35ms TTL=58
Reply from 216.58.210.46: bytes=32 time=57ms TTL=58
Reply from 216.58.210.46: bytes=32 time=38ms TTL=58
Reply from 216.58.210.46: bytes=32 time=34ms TTL=58
Reply from 216.58.210.46: bytes=32 time=69ms TTL=58
Reply from 216.58.210.46: bytes=32 time=121ms TTL=58
Reply from 216.58.210.46: bytes=32 time=58ms TTL=58
Reply from 216.58.210.46: bytes=32 time=68ms TTL=58
Reply from 216.58.210.46: bytes=32 time=80ms TTL=58
Reply from 216.58.210.46: bytes=32 time=82ms TTL=58
Reply from 216.58.210.46: bytes=32 time=23ms TTL=58
Reply from 216.58.210.46: bytes=32 time=43ms TTL=58
Reply from 216.58.210.46: bytes=32 time=22ms TTL=58
Reply from 216.58.210.46: bytes=32 time=79ms TTL=58
Reply from 216.58.210.46: bytes=32 time=67ms TTL=58
Reply from 216.58.210.46: bytes=32 time=40ms TTL=58
Reply from 216.58.210.46: bytes=32 time=22ms TTL=58
Reply from 216.58.210.46: bytes=32 time=57ms TTL=58
Reply from 216.58.210.46: bytes=32 time=43ms TTL=58
Reply from 216.58.210.46: bytes=32 time=76ms TTL=58
Reply from 216.58.210.46: bytes=32 time=68ms TTL=58</code></pre></div><p>and another</p><div class="codebox"><pre><code>Pinging google.com [216.58.208.78] with 32 bytes of data:
Reply from 216.58.208.78: bytes=32 time=23ms TTL=58
Reply from 216.58.208.78: bytes=32 time=63ms TTL=58
Reply from 216.58.208.78: bytes=32 time=35ms TTL=58
Reply from 216.58.208.78: bytes=32 time=32ms TTL=58
Reply from 216.58.208.78: bytes=32 time=26ms TTL=58
Reply from 216.58.208.78: bytes=32 time=32ms TTL=58
Reply from 216.58.208.78: bytes=32 time=129ms TTL=58
Reply from 216.58.208.78: bytes=32 time=77ms TTL=58
Reply from 216.58.208.78: bytes=32 time=56ms TTL=58
Reply from 216.58.208.78: bytes=32 time=39ms TTL=58
Reply from 216.58.208.78: bytes=32 time=143ms TTL=58
Reply from 216.58.208.78: bytes=32 time=46ms TTL=58
Reply from 216.58.208.78: bytes=32 time=74ms TTL=58
Reply from 216.58.208.78: bytes=32 time=47ms TTL=58
Reply from 216.58.208.78: bytes=32 time=60ms TTL=58
Reply from 216.58.208.78: bytes=32 time=83ms TTL=58
Reply from 216.58.208.78: bytes=32 time=48ms TTL=58
Reply from 216.58.208.78: bytes=32 time=71ms TTL=58
Reply from 216.58.208.78: bytes=32 time=45ms TTL=58
Reply from 216.58.208.78: bytes=32 time=42ms TTL=58
Reply from 216.58.208.78: bytes=32 time=60ms TTL=58</code></pre></div><p>With an idle line, pings to google.com are consistent &lt;30ms</p><p>I connect at 4500/448Kbps but have SQM set at 3800/380Kbps. I changed the SQM script to place ICMP in the priority queue which makes testing easier.</p><p>I&#039;m not sure if it&#039;s even possible to achieve any better results with such a slow line, but I am hopefully upgrading soon.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p292826">
				<div class="post-metadata">
					<div class="post-num">Post #9</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						21 Sep 2015, 16:07					</div>
				</div>
				<div class="post-content content">
					<p>Hi Magnetz, <br /></p><div class="quotebox"><cite>Magnetz wrote:</cite><blockquote><p>I don&#039;t actually use netflix but I was having issues with the dota 2 game client which would open a news/blog page every time it was started and download ~20MB over several streams. Which takes a fairly long time on 3.8Mb DSL.</p><p>They have since changed the game client and it no longer does that, but large web pages can still cause ping spikes.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; You only open th client once per game, I assume?<br /></p><div class="quotebox"><cite>Magnetz wrote:</cite><blockquote><p>With an otherwise idle connection this is what happens if I ping google.com while loading dailymail.co.uk</p><div class="codebox"><pre><code>Reply from 216.58.210.46: bytes=32 time=22ms TTL=58
Reply from 216.58.210.46: bytes=32 time=54ms TTL=58
Reply from 216.58.210.46: bytes=32 time=21ms TTL=58
Reply from 216.58.210.46: bytes=32 time=21ms TTL=58
Reply from 216.58.210.46: bytes=32 time=21ms TTL=58
Reply from 216.58.210.46: bytes=32 time=54ms TTL=58
Reply from 216.58.210.46: bytes=32 time=24ms TTL=58
Reply from 216.58.210.46: bytes=32 time=102ms TTL=58
Reply from 216.58.210.46: bytes=32 time=55ms TTL=58
Reply from 216.58.210.46: bytes=32 time=66ms TTL=58
Reply from 216.58.210.46: bytes=32 time=48ms TTL=58
Reply from 216.58.210.46: bytes=32 time=37ms TTL=58
Reply from 216.58.210.46: bytes=32 time=88ms TTL=58
Reply from 216.58.210.46: bytes=32 time=34ms TTL=58
Reply from 216.58.210.46: bytes=32 time=35ms TTL=58
Reply from 216.58.210.46: bytes=32 time=43ms TTL=58
Reply from 216.58.210.46: bytes=32 time=46ms TTL=58
Reply from 216.58.210.46: bytes=32 time=62ms TTL=58
Reply from 216.58.210.46: bytes=32 time=35ms TTL=58
Reply from 216.58.210.46: bytes=32 time=57ms TTL=58
Reply from 216.58.210.46: bytes=32 time=38ms TTL=58
Reply from 216.58.210.46: bytes=32 time=34ms TTL=58
Reply from 216.58.210.46: bytes=32 time=69ms TTL=58
Reply from 216.58.210.46: bytes=32 time=121ms TTL=58
Reply from 216.58.210.46: bytes=32 time=58ms TTL=58
Reply from 216.58.210.46: bytes=32 time=68ms TTL=58
Reply from 216.58.210.46: bytes=32 time=80ms TTL=58
Reply from 216.58.210.46: bytes=32 time=82ms TTL=58
Reply from 216.58.210.46: bytes=32 time=23ms TTL=58
Reply from 216.58.210.46: bytes=32 time=43ms TTL=58
Reply from 216.58.210.46: bytes=32 time=22ms TTL=58
Reply from 216.58.210.46: bytes=32 time=79ms TTL=58
Reply from 216.58.210.46: bytes=32 time=67ms TTL=58
Reply from 216.58.210.46: bytes=32 time=40ms TTL=58
Reply from 216.58.210.46: bytes=32 time=22ms TTL=58
Reply from 216.58.210.46: bytes=32 time=57ms TTL=58
Reply from 216.58.210.46: bytes=32 time=43ms TTL=58
Reply from 216.58.210.46: bytes=32 time=76ms TTL=58
Reply from 216.58.210.46: bytes=32 time=68ms TTL=58</code></pre></div><p>and another</p><div class="codebox"><pre><code>Pinging google.com [216.58.208.78] with 32 bytes of data:
Reply from 216.58.208.78: bytes=32 time=23ms TTL=58
Reply from 216.58.208.78: bytes=32 time=63ms TTL=58
Reply from 216.58.208.78: bytes=32 time=35ms TTL=58
Reply from 216.58.208.78: bytes=32 time=32ms TTL=58
Reply from 216.58.208.78: bytes=32 time=26ms TTL=58
Reply from 216.58.208.78: bytes=32 time=32ms TTL=58
Reply from 216.58.208.78: bytes=32 time=129ms TTL=58
Reply from 216.58.208.78: bytes=32 time=77ms TTL=58
Reply from 216.58.208.78: bytes=32 time=56ms TTL=58
Reply from 216.58.208.78: bytes=32 time=39ms TTL=58
Reply from 216.58.208.78: bytes=32 time=143ms TTL=58
Reply from 216.58.208.78: bytes=32 time=46ms TTL=58
Reply from 216.58.208.78: bytes=32 time=74ms TTL=58
Reply from 216.58.208.78: bytes=32 time=47ms TTL=58
Reply from 216.58.208.78: bytes=32 time=60ms TTL=58
Reply from 216.58.208.78: bytes=32 time=83ms TTL=58
Reply from 216.58.208.78: bytes=32 time=48ms TTL=58
Reply from 216.58.208.78: bytes=32 time=71ms TTL=58
Reply from 216.58.208.78: bytes=32 time=45ms TTL=58
Reply from 216.58.208.78: bytes=32 time=42ms TTL=58
Reply from 216.58.208.78: bytes=32 time=60ms TTL=58</code></pre></div><p>With an idle line, pings to google.com are consistent &lt;30ms</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Thanks for the data. This does not look to bad, but also not as good as one would like. But let&#039;s see how long a single packet occupies your uplink:<br />Packet size: 1514 (I just assume pure ethernet without FCS for simplicity)<br />ATM cells required&quot; ceil(Packet size / ATM cell size) = ceil(1514/48) = ceil(31.5416666667) = 32<br />ATM packet size: 32*53*8&nbsp; = 13568 bit <br />egress Transfer time: 13568/380000 = 0.0357052631579 seconds or 35 milliseconds<br />So a single full sized packet in the tress queue ahead of you ping probe will cause at least 35ms delay if I ran the numbers correctly. Ingress at 3800 only adds 3.5ms though. With your test you most likely will see more full sized packets on ingress than egress.<br /></p><div class="quotebox"><cite>Magnetz wrote:</cite><blockquote><br /><p>I connect at 4500/448Kbps but have SQM set at 3800/380Kbps.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Mmh, could you post the output of the following commands run on your router, please:<br />1) tc -d qdisc<br />2) cat /etc/config/sqm<br />3) /etc/init.d/sqm stop<br />4) /etc/init.d/sqm stop</p><p>I mainly wonder how/whether you configured the link layer adjustments, as for ADSL-links that really helps a lot. Also with correct link layer accounting uplink/egress shaping often works great up to 100% of link rate. But most likely you have set that correctly already. <br />Note correct link layer adjustmented egress shaping at 100% link-rate will not magically create more useable bandwidth. It will however actually treat each packet according to the size the packet will take in the xDSL link and hence allow the shaper to work with less slack. And for ingress even with link layer accounting in use you still will need to reduce to 90 or even 85% of link rate, otherwise the true bottleneck at the ISPs end will fill up more often destroying latency under load...<br />ATM&#039;s 48/53 encapsulation alone will reduce the potential good-put by 9% (100*48/53 = 90.56%) and to this you still need to add the cell quantization and per-packet overhead costs, and you easily end up with IP good-put at 85% of link-rate, so close to your egress shaping rate. Especially the ATM cell quantization is hard to adjust for stochastically as it depends on the size of data+header+per-packet-overhead, and can at small packet sizes reach almost 50%. Or put differently ATM cell quantization can drapg in a full 48 byte cell for even a single byte of payload, and ATM cells are never shared between data packets, but are padded, so the theoretical worst case for ATM cell quantization alone is 49 data bytes in 2*48 bytes worth of cells or 100*49/(2*48) = 51.04%, once you add the 48/53 encapsulation and per packet overhead you start to weep <img src="https://forum.openwrt.org/img/smilies/wink.png" width="15" height="15" alt="wink" />.</p><div class="quotebox"><cite>Magnetz wrote:</cite><blockquote><p>I changed the SQM script to place ICMP in the priority queue which makes testing easier.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Not a bad idea, if you want to test how well packets in the priority queue can tolerate the packet inrush in the other band. One of the reasons SQM does not give ICMP traffic special treatment is so that it is easy to measure the latency under load behavior of the normal best-effort queue.<br /></p><div class="quotebox"><cite>Magnetz wrote:</cite><blockquote><p>I&#039;m not sure if it&#039;s even possible to achieve any better results with such a slow line, but I am hopefully upgrading soon.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Quick note, in case you switch to VDSL2 or FTTC &quot;fiber&quot; (as opposed to FTTH fiber), there is no cell quantization required, and the 64/65 encapsulation can be easily dealt with by t=reducing the shaper rate to &gt;= 100*64/65 = 98.4615384615 of link-rate. (Specifying the correct per-packet-overhead still is helpful, but not as critical as on ATM links).</p><br /><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p293019">
				<div class="post-metadata">
					<div class="post-num">Post #10</div>
					<div class="post-author">Magnetz</div>
					<div class="post-datetime">
						23 Sep 2015, 00:39					</div>
				</div>
				<div class="post-content content">
					<div class="codebox"><pre><code>tc -d qdisc
qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc mq 0: dev wlan1 root
qdisc mq 0: dev wlan0 root
qdisc htb 1: dev pppoe-wan root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 3
 linklayer atm overhead 44 mtu 2047 tsize 512
qdisc fq_codel 110: dev pppoe-wan parent 1:11 limit 1001p flows 1024 quantum 300 target 32.4ms interval 50.0ms
qdisc fq_codel 120: dev pppoe-wan parent 1:12 limit 1001p flows 1024 quantum 300 target 32.4ms interval 50.0ms
qdisc fq_codel 130: dev pppoe-wan parent 1:13 limit 1001p flows 1024 quantum 300 target 32.4ms interval 50.0ms
qdisc ingress ffff: dev pppoe-wan parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4pppoe-wan root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer atm overhead 44 mtu 2047 tsize 512
qdisc fq_codel 110: dev ifb4pppoe-wan parent 1:10 limit 1001p flows 1024 quantum 300 target 5.0ms interval 50.0ms ecn</code></pre></div><div class="codebox"><pre><code>cat /etc/config/sqm

config queue &#039;eth1&#039;
        option interface &#039;pppoe-wan&#039;
        option overhead &#039;44&#039;
        option linklayer &#039;atm&#039;
        option enabled &#039;1&#039;
        option qdisc &#039;fq_codel&#039;
        option download &#039;3800&#039;
        option upload &#039;380&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option iqdisc_opts &#039;interval 50ms&#039;
        option eqdisc_opts &#039;interval 50ms&#039;
        option script &#039;simple.qos&#039;</code></pre></div><div class="codebox"><pre><code>/etc/init.d/sqm stop
SQM: Trying to start/stop SQM on all interfaces.
SQM: run.sh stop
SQM: /usr/lib/sqm/run.sh Stopping SQM on interface: pppoe-wan
SQM: ifb associated with interface pppoe-wan: ifb4pppoe-wan
SQM: /usr/lib/sqm/stop.sh: Stopping pppoe-wan
SQM: ifb associated with interface pppoe-wan: ifb4pppoe-wan
SQM: /usr/lib/sqm/stop.sh: ifb4pppoe-wan shaper deleted
SQM: /usr/lib/sqm/stop.sh: ifb4pppoe-wan interface deleted
SQM: /usr/lib/sqm/run.sh SQM qdiscs on pppoe-wan removed</code></pre></div><div class="codebox"><pre><code>/etc/init.d/sqm stop
SQM: Trying to start/stop SQM on all interfaces.
SQM: run.sh stop
SQM: /usr/lib/sqm/run.sh SQM qdiscs on pppoe-wan removed</code></pre></div><p>and just for luck <em>sqm start</em><br /></p><div class="codebox"><pre><code> /etc/init.d/sqm start
SQM: Trying to start/stop SQM on all interfaces.
SQM: /usr/lib/sqm/run.sh Queue Setup Script: /usr/lib/sqm/simple.qos
SQM: ifb associated with interface pppoe-wan:
SQM: trying to create new IFB: ifb4pppoe-wan
Failed to find sch_fq_codel. Maybe it is a built in module ?
SQM: Squashing differentiated services code points (DSCP) from ingress.
SQM: STAB: stab mtu 2047 tsize 512 mpu 0 overhead 44 linklayer atm
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: egress shaping activated
SQM: Do not perform DSCP based filtering on ingress. (1-tier classification)
SQM: STAB: stab mtu 2047 tsize 512 mpu 0 overhead 44 linklayer atm
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: ingress shaping activated</code></pre></div><p>The dota 2 game client was actually causing a problem when 1 person was playing a game and another person opened the dota 2 client on a different pc.</p>									</div>
			</article>

			
		
	
			<div class="notice minor">
			<p>The discussion might have continued from here.</p>
		</div>
	
	<div class="pagination"><div class="pagination-number">Page 1 of 1</div><nav><ul><li class="pagination-current"><span>1</span></li></ul></nav></div>
</main>

</div>


<!-- Created in a hurry and not indicative of usual code quality. Here's a number: 0 -->

</body>
</html>