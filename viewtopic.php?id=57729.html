<!DOCTYPE html>
<html lang="en-US">
<head>

	<title>OpenWrt Forum Archive</title>

	<meta charset="UTF-8">

	<meta http-equiv="X-UA-Compatible" content="IE=edge">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="stylesheet" href="assets/css/common.css">

</head>
<body>

<div class="container">

<header class="main-header">
	<h1 class="logo"><a href="index.html"><img src="assets/img/logo.png" width="376" height="88" alt="OpenWrt Forum Archive"></a></h1>
</header>

<aside>
	<p>This is a read-only archive of the old OpenWrt forum. The current OpenWrt forum resides at <a href="https://forum.openwrt.org/">https://forum.openwrt.org/</a>.</p>
	<p class="minor">In May 2018, the OpenWrt forum suffered a total data loss. This archive is an effort to restore and make available as much content as possible. Content may be missing or not representing the latest edited version.</p>
</aside>

<main>
	<header>
		<h1><span class="minor">Topic:</span> SQM Throttling Speeds To Much</h1>
	</header>
	<div class="notice minor">
		<p>
			The content of this topic has been archived
							between 10 Apr 2018 and 19 Apr 2018.
										There are no obvious gaps in this topic, but there may still be some posts missing at the end.
					</p>
	</div>

	<div class="pagination"><div class="pagination-number">Page 1 of 2</div><nav><ul><li class="pagination-current"><span>1</span></li><li><a href="viewtopic.php%3Fid=57729&amp;p=2.html">2</a></li></ul></nav></div>
			
		
		
			<article class="post" id="p278526">
				<div class="post-metadata">
					<div class="post-num">Post #1</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						2 Jun 2015, 02:29					</div>
				</div>
				<div class="post-content content">
					<p>Long story short, my router has been suffering from bufferbloating and I used SQM to help with that issue. The SQM does help with the bufferbloat, but it throttles my download and upload speeds way more than I&#039;d like. I get a 13.5 mbs. down and about 3 up. Once I set the QoS to 12225 down and 2150 up my speeds start to drop to 9-10 Mbs down and 1.8 up. I don&#039;t expect my speeds to be exactly what I set them to, but a 2 mbs drop seems a bit drastic. Is there anyway I can get rid of bufferbloating without sacrificing 3 mbs?</p><p>Here are my speedtests:</p><p>QoS off</p><p>Ookla - <a href="http://www.speedtest.net/my-result/4402214899">http://www.speedtest.net/my-result/4402214899</a><br />DSL - <a href="http://www.dslreports.com/speedtest/595024">http://www.dslreports.com/speedtest/595024</a></p><p>QoS on</p><p>Ookla - <a href="http://www.speedtest.net/my-result/4402065146">http://www.speedtest.net/my-result/4402065146</a><br />DSL - <a href="http://www.dslreports.com/speedtest/595010">http://www.dslreports.com/speedtest/595010</a></p><p>I checked at other times of the day, and the speeds are similar.&nbsp; I use the WNDR 4300 v1 with Barrier Breaker 14.07 firmware installed.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278546">
				<div class="post-metadata">
					<div class="post-num">Post #2</div>
					<div class="post-author">alphasparc</div>
					<div class="post-datetime">
						2 Jun 2015, 09:07					</div>
				</div>
				<div class="post-content content">
					<p>How else can the latency be improved if no bandwidth is reserved?<br />You can increase the SQM value but eventually you still need to reserve some bandwidth.</p>											<p class="post-edited">(Last edited by <strong>alphasparc</strong> on 2 Jun 2015, 09:08)</p>
									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278597">
				<div class="post-metadata">
					<div class="post-num">Post #3</div>
					<div class="post-author">richbhanover</div>
					<div class="post-datetime">
						2 Jun 2015, 16:49					</div>
				</div>
				<div class="post-content content">
					<p>@Panny P - I&#039;m also reading this list and will respond on <a href="http://www.techsupportforum.com/forums/f31/bufferbloat-wont-go-away-997842.html">http://www.techsupportforum.com/forums/ … 97842.html</a> when I learn more.</p>											<p class="post-edited">(Last edited by <strong>richbhanover</strong> on 2 Jun 2015, 17:11)</p>
									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278605">
				<div class="post-metadata">
					<div class="post-num">Post #4</div>
					<div class="post-author">richbhanover</div>
					<div class="post-datetime">
						2 Jun 2015, 17:26					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>alphasparc wrote:</cite><blockquote><p>How else can the latency be improved if no bandwidth is reserved?<br />You can increase the SQM value but eventually you still need to reserve some bandwidth.</p></blockquote></div><p>The beauty of the SQM QoS (luci-app-sqm package) is that it has almost no configuration - only the link speeds and the type of link (choose between ATM/DSL vs nearly every other type of link). Traditional QoS/reservation/prioritization schemes require fiddling (&quot;make this traffic better than that...&quot;) and need revision if your needs/applications change.</p><p>On the other hand, the fq_codel algorithm reviews the amount of data queued for each flow/connection, and only throttles those that are using more than their fair share. (It does this by measuring the &quot;sojourn time&quot; of each packet in the queue. If the packets have been queued too long, fq_codel begins to exert flow control on that session alone.)</p><p>This allows sparse flows (such as DNS lookups, and TCP connect packets) and low-volume flows (such as VoIP and gaming) to be sent immediately, while allowing the other &quot;elephant&quot; flows to share the remaining bandwidth. For more information, see <a href="http://www.bufferbloat.net/projects/cerowrt/wiki/What_to_do_about_Bufferbloat">http://www.bufferbloat.net/projects/cer … ufferbloat</a></p>											<p class="post-edited">(Last edited by <strong>richbhanover</strong> on 2 Jun 2015, 17:28)</p>
									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278611">
				<div class="post-metadata">
					<div class="post-num">Post #5</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						2 Jun 2015, 17:45					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>Long story short, my router has been suffering from bufferbloating and I used SQM to help with that issue. The SQM does help with the bufferbloat, but it throttles my download and upload speeds way more than I&#039;d like. I get a 13.5 mbs. down and about 3 up. Once I set the QoS to 12225 down and 2150 up my speeds start to drop to 9-10 Mbs down and 1.8 up. I don&#039;t expect my speeds to be exactly what I set them to, but a 2 mbs drop seems a bit drastic. Is there anyway I can get rid of bufferbloating without sacrificing 3 mbs?</p><p>Here are my speedtests:</p><p>QoS off</p><p>Ookla - <a href="http://www.speedtest.net/my-result/4402214899">http://www.speedtest.net/my-result/4402214899</a><br />DSL - <a href="http://www.dslreports.com/speedtest/595024">http://www.dslreports.com/speedtest/595024</a></p><p>QoS on</p><p>Ookla - <a href="http://www.speedtest.net/my-result/4402065146">http://www.speedtest.net/my-result/4402065146</a><br />DSL - <a href="http://www.dslreports.com/speedtest/595010">http://www.dslreports.com/speedtest/595010</a></p><p>I checked at other times of the day, and the speeds are similar.&nbsp; I use the WNDR 4300 v1 with Barrier Breaker 14.07 firmware installed.</p></blockquote></div><p>Hi Panny P,</p><p>what kind of link are you using, xDSL or cable? And how did you configure SQM. If you could post the output of the following commands (just the stuff inside the double quotes) run on your router we have a bit more information to go on:<br />1) &quot;tc -d qdisc&quot;</p><p>2) &quot;ifconfig&quot;</p><p>3) &quot;etc/init.d/sqm stop ; /etc/init.d/sqm start&quot;</p><br /><p>If you have a DSL link, what ISP are you using and what are the synch rates on your line?</p><p>(Ans alphasparc is right again, you will have to sacrifice some bandwidth to regain decent control over the latency under load, on ingress/download typically 10-20% on egress/upload 2-10%, you will need a few trials to find the sweet spot of your link...)</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278643">
				<div class="post-metadata">
					<div class="post-num">Post #6</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						2 Jun 2015, 21:47					</div>
				</div>
				<div class="post-content content">
					<p>Hi,</p><p>I forgot to mention the output of the following command would also be interesting:<br />1) &quot;cat /etc/config/sqm&quot;</p><p>Following Rich Brown&#039;s link a bit: sqm will not deal well with variable bandwidth as caused by oversubscription of a tower&#039;s uplink. I believe gargoyle has some adaptive qos method that might allow to track the available bandwidth better and keep latency low while only sacrificing as much bandwidth as &quot;currently&quot; necessary (which will fluctuate during the day, inversely following the upstream congestion). But I have&nbsp; never tried gargoyle myself so it might work better or worse than my description makes it sound...</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278686">
				<div class="post-metadata">
					<div class="post-num">Post #7</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						3 Jun 2015, 02:56					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>alphasparc wrote:</cite><blockquote><p>How else can the latency be improved if no bandwidth is reserved?<br />You can increase the SQM value but eventually you still need to reserve some bandwidth.</p></blockquote></div><p>I understand that I have to reserve bandwidth, but my problem is that the SQM is throttling more than what I set it to.<br />I&#039;m using the 90% rule, but instead the QoS is throttling more around 25% of my bandwidth instead of the 10% I set it to. <br /></p><div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p>Hi Panny P,</p><p>what kind of link are you using, xDSL or cable? And how did you configure SQM. If you could post the output of the following commands (just the stuff inside the double quotes) run on your router we have a bit more information to go on:<br />1) &quot;tc -d qdisc&quot;</p><p>2) &quot;ifconfig&quot;</p><p>3) &quot;etc/init.d/sqm stop ; /etc/init.d/sqm start&quot;</p><br /><p>If you have a DSL link, what ISP are you using and what are the synch rates on your line?</p><p>(Ans alphasparc is right again, you will have to sacrifice some bandwidth to regain decent control over the latency under load, on ingress/download typically 10-20% on egress/upload 2-10%, you will need a few trials to find the sweet spot of your link...)</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p></blockquote></div><p>I&#039;m using the WISP type, with the satellite sending the signal to a nearby tower. Speeds can by shaky with WISP, but I am within walking distance of the tower with a clear line of site to it. My connection is stable to the ISP.</p><p>As for configuring the SQM I followed these instructions. <a href="http://wiki.openwrt.org/doc/howto/sqm">http://wiki.openwrt.org/doc/howto/sqm</a> I triple checked how I set up the SQM to.</p><p>I would like to give you the test results, but I am not sure how to run them. I found this article <a href="http://wiki.openwrt.org/doc/howto/user.beginner.cli">http://wiki.openwrt.org/doc/howto/user.beginner.cli</a> but I can&#039;t seem to figure it out.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p278927">
				<div class="post-metadata">
					<div class="post-num">Post #8</div>
					<div class="post-author">richbhanover</div>
					<div class="post-datetime">
						5 Jun 2015, 07:19					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>I&#039;m using the WISP type, with the satellite sending the signal to a nearby tower. Speeds can by shaky with WISP, but I am within walking distance of the tower with a clear line of site to it. My connection is stable to the ISP.</p></blockquote></div><p>Ahah! I just understood what you have been telling me: you&#039;re not actually using a satellite link, but fixed broadband wireless through a WISP. The dish at your house points to an antenna at the tower, and you basically have a high-speed wireless link to that tower. Consequently, your base latencies will be low (certainly not satellite delays), and the DSL choice for DSLReports Speed Test should give fair measurements. </p><div class="quotebox"><blockquote><p>As for configuring the SQM I followed these instructions. <a href="http://wiki.openwrt.org/doc/howto/sqm">http://wiki.openwrt.org/doc/howto/sqm</a> I triple checked how I set up the SQM to.</p><p>I would like to give you the test results, but I am not sure how to run them. I found this article <a href="http://wiki.openwrt.org/doc/howto/user.beginner.cli">http://wiki.openwrt.org/doc/howto/user.beginner.cli</a> but I can&#039;t seem to figure it out.</p></blockquote></div><p>You&#039;ll need to use a terminal program to create a SSH session to the router and enter those commands. On a Unix/Linux/OSX, find the Terminal application and type:</p><p>ssh root@192.168.1.1</p><p>On Windows, you&#039;ll need a &quot;SSH Client&quot;. The long-loved PuTTY can be found on <a href="http://www.putty.org">www.putty.org</a>. There are a zillion tutorials on the web, but you want to use PuTTY to establish a SSH session to 192.168.1.1, with the username &quot;root&quot; and the password that you assigned to the router.</p><p>Once you&#039;ve connected in, you should see a command-line prompt of &quot;root@OpenWrt:~:&quot;</p><p>Finally, enter the commands that moeller0 suggested, and copy/paste the entire response.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p279246">
				<div class="post-metadata">
					<div class="post-num">Post #9</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						8 Jun 2015, 01:44					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p>Hi Panny P,</p><p>what kind of link are you using, xDSL or cable? And how did you configure SQM. If you could post the output of the following commands (just the stuff inside the double quotes) run on your router we have a bit more information to go on:<br />1) &quot;tc -d qdisc&quot;</p><p>2) &quot;ifconfig&quot;</p><p>3) &quot;etc/init.d/sqm stop ; /etc/init.d/sqm start&quot;</p></blockquote></div><p>Okay, here are the results. For what it&#039;s worth I ran these while QoS was off and my internet is currently NOT fluctuating in speeds or anything bad.</p><p>1) </p><div class="codebox"><pre><code> qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 5.7ms interval 100.7ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 5.7ms interval 100.7ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 5.7ms interval 100.7ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc mq 0: dev wlan0 root
qdisc mq 0: dev wlan1 root
qdisc mq 0: dev wlan1-1 root
qdisc mq 0: dev wlan1-2 root
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn </code></pre></div><p>2) </p><div class="codebox"><pre><code> br-lan    Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7B
          inet addr:192.168.1.1  Bcast:192.168.1.255  Mask:255.255.255.0
          inet6 addr: fe80::e6f4:c6ff:feeb:e87b/64 Scope:Link
          inet6 addr: fd77:5df5:258c::1/60 Scope:Global
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:3590681 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6311486 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:261692214 (249.5 MiB)  TX bytes:9051014512 (8.4 GiB)

eth0      Link encap:Ethernet  HWaddr 52:B7:2B:7C:2D:D9
          inet6 addr: fe80::50b7:2bff:fe7c:2dd9/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:8939501 errors:0 dropped:6 overruns:0 frame:0
          TX packets:7860252 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:745476973 (710.9 MiB)  TX bytes:2127420026 (1.9 GiB)
          Interrupt:4

eth0.1    Link encap:Ethernet  HWaddr 52:B7:2B:7C:2D:D9
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2469299 errors:0 dropped:53 overruns:0 frame:0
          TX packets:4259429 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:185141890 (176.5 MiB)  TX bytes:6088130197 (5.6 GiB)

eth0.2    Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7C
          inet addr:67.43.124.117  Bcast:67.43.124.127  Mask:255.255.255.192
          inet6 addr: fe80::e6f4:c6ff:feeb:e87c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:6470156 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3600817 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:8989353142 (8.3 GiB)  TX bytes:302815609 (288.7 MiB)

ifb4eth0.2 Link encap:Ethernet  HWaddr A2:7A:87:B0:A3:FF
          inet6 addr: fe80::a07a:87ff:feb0:a3ff/64 Scope:Link
          UP BROADCAST RUNNING NOARP  MTU:1500  Metric:1
          RX packets:6457202 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6457202 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:32
          RX bytes:9087964229 (8.4 GiB)  TX bytes:9087964229 (8.4 GiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:48548 errors:0 dropped:0 overruns:0 frame:0
          TX packets:48548 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:3328158 (3.1 MiB)  TX bytes:3328158 (3.1 MiB)

wlan0     Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7B
          inet6 addr: fe80::e6f4:c6ff:feeb:e87b/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1005796 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1929848 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:76685694 (73.1 MiB)  TX bytes:2836423341 (2.6 GiB)

wlan1     Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7D
          inet6 addr: fe80::e6f4:c6ff:feeb:e87d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16385 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:2059386 (1.9 MiB)

wlan1-1   Link encap:Ethernet  HWaddr E6:F4:C6:EB:E8:7D
          inet6 addr: fe80::e4f4:c6ff:feeb:e87d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:137776 errors:0 dropped:0 overruns:0 frame:0
          TX packets:170803 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:20753957 (19.7 MiB)  TX bytes:174492508 (166.4 MiB)

wlan1-2   Link encap:Ethernet  HWaddr E2:F4:C6:EB:E8:7D
          inet6 addr: fe80::e0f4:c6ff:feeb:e87d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16357 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:2055020 (1.9 MiB) </code></pre></div><p>3)&nbsp; </p><div class="codebox"><pre><code> -ash: etc/init.d/sqm: not found
SQM: Trying to start/stop SQM on all interfaces.
SQM: /usr/lib/sqm/run.sh Stopping SQM on interface: eth0.2
SQM: ifb associated with interface eth0.2: ifb4eth0.2
SQM: /usr/lib/sqm/stop.sh: Stopping eth0.2
SQM: ifb associated with interface eth0.2: ifb4eth0.2
SQM: /usr/lib/sqm/stop.sh: ifb4eth0.2 shaper deleted
SQM: /usr/lib/sqm/stop.sh: ifb4eth0.2 interface deleted
SQM: /usr/lib/sqm/run.sh Queue Setup Script: /usr/lib/sqm/simple.qos
SQM: ifb associated with interface eth0.2:
SQM: trying to create new IFB: ifb4eth0.2
Failed to find sch_fq_codel. Maybe it is a built in module ?
SQM: Squashing differentiated services code points (DSCP) from ingress.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: egress shaping activated
SQM: Do not perform DSCP based filtering on ingress. (1-tier classification)
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: ingress shaping activated </code></pre></div>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p279269">
				<div class="post-metadata">
					<div class="post-num">Post #10</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						8 Jun 2015, 13:34					</div>
				</div>
				<div class="post-content content">
					<p>Hi Panny P.</p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p>Hi Panny P,</p><p>what kind of link are you using, xDSL or cable? And how did you configure SQM. If you could post the output of the following commands (just the stuff inside the double quotes) run on your router we have a bit more information to go on:<br />1) &quot;tc -d qdisc&quot;</p><p>2) &quot;ifconfig&quot;</p><p>3) &quot;etc/init.d/sqm stop ; /etc/init.d/sqm start&quot;</p></blockquote></div><p>Okay, here are the results. For what it&#039;s worth I ran these while QoS was off and my internet is currently NOT fluctuating in speeds or anything bad.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Thanks a lot.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>1) </p><div class="codebox"><pre><code> qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 5.7ms interval 100.7ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 5.7ms interval 100.7ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 5.7ms interval 100.7ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc mq 0: dev wlan0 root
qdisc mq 0: dev wlan1 root
qdisc mq 0: dev wlan1-1 root
qdisc mq 0: dev wlan1-2 root
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn </code></pre></div></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Mmmh, the existence of the two &quot;qdisc htb 1: dev *&quot; lines indicates that SQM actually was active, yet you state above that it was off. Could you clarify what you meant?<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><br /><p>2) </p><div class="codebox"><pre><code> br-lan    Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7B
          inet addr:192.168.1.1  Bcast:192.168.1.255  Mask:255.255.255.0
          inet6 addr: fe80::e6f4:c6ff:feeb:e87b/64 Scope:Link
          inet6 addr: fd77:5df5:258c::1/60 Scope:Global
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:3590681 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6311486 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:261692214 (249.5 MiB)  TX bytes:9051014512 (8.4 GiB)

eth0      Link encap:Ethernet  HWaddr 52:B7:2B:7C:2D:D9
          inet6 addr: fe80::50b7:2bff:fe7c:2dd9/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:8939501 errors:0 dropped:6 overruns:0 frame:0
          TX packets:7860252 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:745476973 (710.9 MiB)  TX bytes:2127420026 (1.9 GiB)
          Interrupt:4

eth0.1    Link encap:Ethernet  HWaddr 52:B7:2B:7C:2D:D9
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2469299 errors:0 dropped:53 overruns:0 frame:0
          TX packets:4259429 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:185141890 (176.5 MiB)  TX bytes:6088130197 (5.6 GiB)

eth0.2    Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7C
          inet addr:67.43.124.117  Bcast:67.43.124.127  Mask:255.255.255.192
          inet6 addr: fe80::e6f4:c6ff:feeb:e87c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:6470156 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3600817 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:8989353142 (8.3 GiB)  TX bytes:302815609 (288.7 MiB)

ifb4eth0.2 Link encap:Ethernet  HWaddr A2:7A:87:B0:A3:FF
          inet6 addr: fe80::a07a:87ff:feb0:a3ff/64 Scope:Link
          UP BROADCAST RUNNING NOARP  MTU:1500  Metric:1
          RX packets:6457202 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6457202 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:32
          RX bytes:9087964229 (8.4 GiB)  TX bytes:9087964229 (8.4 GiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:48548 errors:0 dropped:0 overruns:0 frame:0
          TX packets:48548 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:3328158 (3.1 MiB)  TX bytes:3328158 (3.1 MiB)

wlan0     Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7B
          inet6 addr: fe80::e6f4:c6ff:feeb:e87b/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1005796 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1929848 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:76685694 (73.1 MiB)  TX bytes:2836423341 (2.6 GiB)

wlan1     Link encap:Ethernet  HWaddr E4:F4:C6:EB:E8:7D
          inet6 addr: fe80::e6f4:c6ff:feeb:e87d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16385 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:2059386 (1.9 MiB)

wlan1-1   Link encap:Ethernet  HWaddr E6:F4:C6:EB:E8:7D
          inet6 addr: fe80::e4f4:c6ff:feeb:e87d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:137776 errors:0 dropped:0 overruns:0 frame:0
          TX packets:170803 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:20753957 (19.7 MiB)  TX bytes:174492508 (166.4 MiB)

wlan1-2   Link encap:Ethernet  HWaddr E2:F4:C6:EB:E8:7D
          inet6 addr: fe80::e0f4:c6ff:feeb:e87d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16357 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:2055020 (1.9 MiB) </code></pre></div><p>3)&nbsp; </p><div class="codebox"><pre><code> -ash: etc/init.d/sqm: not found

[/quote]
        Sorry, it should have read &quot;/etc/init.d/sqm stop ; /etc/init.d/sqm start&quot;
[quote=Panny P]

SQM: Trying to start/stop SQM on all interfaces.
SQM: /usr/lib/sqm/run.sh Stopping SQM on interface: eth0.2
SQM: ifb associated with interface eth0.2: ifb4eth0.2
SQM: /usr/lib/sqm/stop.sh: Stopping eth0.2
SQM: ifb associated with interface eth0.2: ifb4eth0.2
SQM: /usr/lib/sqm/stop.sh: ifb4eth0.2 shaper deleted
SQM: /usr/lib/sqm/stop.sh: ifb4eth0.2 interface deleted
SQM: /usr/lib/sqm/run.sh Queue Setup Script: /usr/lib/sqm/simple.qos
SQM: ifb associated with interface eth0.2:
SQM: trying to create new IFB: ifb4eth0.2
Failed to find sch_fq_codel. Maybe it is a built in module ?

[/quote]
        This is not a bug it is just a bit annoying...
[quote=Panny P]

SQM: Squashing differentiated services code points (DSCP) from ingress.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: egress shaping activated
SQM: Do not perform DSCP based filtering on ingress. (1-tier classification)
SQM: get_limit: CURLIMIT: 1001
SQM: get_target defaulting to auto.
SQM: ingress shaping activated </code></pre></div></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; So this looks all okay, I guess I also need the output of:<br />&quot;cat /etc/config/sqm&quot;<br />and information what link rates your WISP-modem reports to use with its &quot;tower&quot;, are these fluctuating over the day.<br />If the WISP connection is stable can you try to increase the rates for the shaper in small (100kbps) steps and test at what rate the bufferbloat starts getting bad? It would probably also help testing with both ingress and egress rates set to 50% of what the WISP-modem reports to see what best case expectancy is. Note that the &quot;idle bar&quot; in the <a href="http://www.dslreports.com/speedtest">http://www.dslreports.com/speedtest</a> results will expand to a display showing the RTT of all individual probes sent already, and there it is obvious that even without load your link &quot;wobbles&quot; a lot (all these should be pretty close to the minimum of ~20ms instead they reach 117ms without QoS and 60ms with). And even with QoS active the RTTs jump around quite a bit. <br />&nbsp; &nbsp; &nbsp; &nbsp; I believe you have done all these tests with the shaper set to &quot;12225Kbps down and 2150kbps up&quot;, correct? Assuming standard ethernet frames of 1514 Byte and the use of tcp (20 byte overhead) over IPv4 (another 20 Byte overhead), you can expect an overhead cost of 100-100*(1500-20-20)/1514 = 3.57% if your WISP link uses other overheads (like VLAN tags) or reports the link rate not in raw &quot;ethernet&quot; terms the cost will be larger </p><p>At 12225Kbps you can expect: (12225*1000 * ((1500-20-20)/1514))/1000 = 11788.97 Kbps <br />At 2150 you can expect:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (2150*1000 * ((1500-20-20)/1514))/1000 = 2073.32 Kbps</p><p>So the reported 9250 Kbps (or 100*9250/11788.97 = 78.46%) and 1790 Kbps (or 100*1790/2073.32 = 86.3349603534) from dslreports seem odd. The tests with QoS disabled are close enough to your assumed 15/3 Mbps service that we probably can rule out other background traffic eating up bandwidth during the test. So I am at a loss figuring out why your bandwidth is lost. Once thing you could try, is to (first get a free account at dslreports unless you have one already) configure <a href="http://www.dslreports.com/speedtest">http://www.dslreports.com/speedtest</a> to use more concurrent streams (like 10/10, you probably need to try a few combination), as that should even out unhappiness of the tcp streams with the &quot;lumpy&quot; WISP link.</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p279328">
				<div class="post-metadata">
					<div class="post-num">Post #11</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						8 Jun 2015, 22:17					</div>
				</div>
				<div class="post-content content">
					<p>Hi Panny P.</p><p>I just had a look at the website of wiser, which I assume to be your ISP. It seems they run PPPoE over the link so you probably should specify an overhead of 8byte for sqm-scripts (not that this explains the observed bandwidth loss). Also have a look at <a href="http://www.speedguide.net/analyzer.php">http://www.speedguide.net/analyzer.php</a> and report the detected MTU value (due to pppoe the MTU should be 1492).</p><p>That changes the calculation of the expected rates a tiny bit:<br />Assuming standard ethernet frames of 1514 Byte and the use of tcp (20 byte overhead) over IPv4 (another 20 Byte overhead), you can expect an overhead cost of 100-100*(1500-20-20-8)/1514 = 4.1 % if your WISP link uses other overheads (like VLAN tags) or reports the link rate not in raw &quot;ethernet&quot; terms the cost will be larger<br />At 12225Kbps you can expect: (12225*1000 * ((1500-20-20-8)/1514))/1000 = 11724.37 Kbps <br />At 2150 you can expect:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (2150*1000 * ((1500-20-20-8)/1514))/1000 = 2062 Kbps</p><p>So the reported 9250 Kbps (or 100*9250/11724.37 = 78.9%) and 1790 Kbps (or 100*1790/2062 = 86.33%) from dslreports still seem odd...</p><p>But again this is not where your loss is coming from.</p><br /><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>											<p class="post-edited">(Last edited by <strong>moeller0</strong> on 8 Jun 2015, 23:04)</p>
									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p279360">
				<div class="post-metadata">
					<div class="post-num">Post #12</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						9 Jun 2015, 02:44					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><blockquote><p>Mmmh, the existence of the two &quot;qdisc htb 1: dev *&quot; lines indicates that SQM actually was active, yet you state above that it was off. Could you clarify what you meant?</p></blockquote></div><p>I goofed, I backed openwrt up to an earlier save state before running those tests, which apparently had QoS enabled. I should have double checked.</p><div class="quotebox"><blockquote><p>So this looks all okay, I guess I also need the output of:<br />&quot;cat /etc/config/sqm&quot;</p></blockquote></div><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option qdisc_advanced &#039;0&#039;
        option linklayer &#039;none&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option enabled &#039;0&#039; </code></pre></div><br /><div class="quotebox"><blockquote><p>...and information what link rates your WISP-modem reports to use with its &quot;tower&quot;, are these fluctuating over the day.<br />If the WISP connection is stable can you try to increase the rates for the shaper in small (100kbps) steps and test at what rate the bufferbloat starts getting bad? It would probably also help testing with both ingress and egress rates set to 50% of what the WISP-modem reports to see what best case expectancy is.</p></blockquote></div><p>The fluctuations are on my ISP&#039;s end during peak hours. I do all the testing when the speeds are what I would be getting during peak hours ideally.&nbsp; I found 12400 to be the best shaper rate. 12500 the ms skyrockets. </p><p>For the ingress/egress rates; how do I find the best case expectancy? Also I am not sure how to set said rates.</p><div class="quotebox"><blockquote><p>configure <a href="http://www.dslreports.com/speedtest">http://www.dslreports.com/speedtest</a> to use more concurrent streams (like 10/10, you probably need to try a few combination), as that should even out unhappiness of the tcp streams with the &quot;lumpy&quot; WISP link.</p></blockquote></div><p>I messed with a few types and found 10/10 to work the best. I ran the test twice ( about 5 minutes apart) and the idle MS ranged from 20 up to 90 only once. Any others I tested and it went up to 150 and beyond.</p><br /><p>Annnd here is the speedguide data.</p><div class="codebox"><pre><code> « SpeedGuide.net TCP Analyzer Results » 
Tested on: 2015.06.08 19:39 
IP address: 67.43.xxx.xxx 
Client OS/browser: Windows 8.1 (Firefox 38.0) 
 
TCP options string: 020405b40103030801010402 
MSS: 1460 
MTU: 1500 
TCP Window: 65536 (NOT multiple of MSS) 
RWIN Scaling: 8 bits (2^8=256) 
Unscaled RWIN : 256 
Recommended RWINs: 64240, 128480, 256960, 513920, 1027840 
BDP limit (200ms): 2621kbps (328KBytes/s)
BDP limit (500ms): 1049kbps (131KBytes/s) 
MTU Discovery: ON 
TTL: 112 
Timestamps: OFF 
SACKs: ON 
IP ToS: 00101000 (40) 
    Precedence: 001 (priority)
    Delay: 0 (normal delay)
    Throughput: 1 (high throughput)
    Reliability: 0 (normal reliability)
    Cost: 0 (normal cost)
    Check bit: 0 (correct)
DSCP (DiffServ): AF11 001010 (10) - Assured Forwarding class 1, low drop precedence (RFC 2597). </code></pre></div><p>I am assuming that getting the data does not require me to download anything. I just connected to the site and copied the &quot;my results&quot; section.</p><p>I noticed you went to &quot;wiser&quot; website. My ISP is Wisper. Here is a link to their site if you want to check it out. <a href="http://www.wisperisp.com/wp/">http://www.wisperisp.com/wp/</a></p><div class="quotebox"><blockquote><p>you probably should specify an overhead of 8byte for sqm-scripts</p></blockquote></div><p>lastly, in the QoS overhead settings, would I set it to &quot;ATM&quot; or &quot;Ethernet with overhead:&quot;? The openwrt wiki article doesn&#039;t specify satellite or Wisp type connections.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p279391">
				<div class="post-metadata">
					<div class="post-num">Post #13</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						9 Jun 2015, 10:40					</div>
				</div>
				<div class="post-content content">
					<p>Hi Panny P,</p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><div class="quotebox"><blockquote><p>Mmmh, the existence of the two &quot;qdisc htb 1: dev *&quot; lines indicates that SQM actually was active, yet you state above that it was off. Could you clarify what you meant?</p></blockquote></div><p>I goofed, I backed openwrt up to an earlier save state before running those tests, which apparently had QoS enabled. I should have double checked.</p><div class="quotebox"><blockquote><p>So this looks all okay, I guess I also need the output of:<br />&quot;cat /etc/config/sqm&quot;</p></blockquote></div><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option qdisc_advanced &#039;0&#039;
        option linklayer &#039;none&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option enabled &#039;0&#039; </code></pre></div></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Okay, so this looks like QoS is off this time.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><div class="quotebox"><blockquote><p>...and information what link rates your WISP-modem reports to use with its &quot;tower&quot;, are these fluctuating over the day.<br />If the WISP connection is stable can you try to increase the rates for the shaper in small (100kbps) steps and test at what rate the bufferbloat starts getting bad? It would probably also help testing with both ingress and egress rates set to 50% of what the WISP-modem reports to see what best case expectancy is.</p></blockquote></div><p>The fluctuations are on my ISP&#039;s end during peak hours.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; So sqm-scripts can only worn effectively if it has control over the (artificial) bottleneck, otherwise latency under load increases badly (depending on the bufferbloat of upstream equipment). If the wireless link rates to your ISP stay reasonable stable during the day, just pick the minimum of those rates to base your shaper rate on (like 90% of the minimum; obviously if the link is lost the minimum will be zero, so pick a reasonable minimum <img src="https://forum.openwrt.org/img/smilies/wink.png" width="15" height="15" alt="wink" /> ). But the buffering at the ISPs end do not explain, why the shaper causes such large a bandwidth sacrifice.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>I do all the testing when the speeds are what I would be getting during peak hours ideally.&nbsp; I found 12400 to be the best shaper rate. 12500 the ms skyrockets. </p><p>For the ingress/egress rates; how do I find the best case expectancy? Also I am not sure how to set said rates.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; From the router&#039;s perspective all incoming traffic is &quot;ingress&quot; all outgoing traffic is &quot;egress&quot;, or from the user&#039;s perspective: download + ingress, upload = egress. You just put in the rates you calculated into the two bandwidth entry boxes in the SQM guy. I think you already did this. About what rates are reasonable; actually I have no experience with WISP, for DSL typically 90% of the dowload-synch/throttle rate and up to 100% of the uplink-synch/throttle rate. I guess you will have to find working numbers for your WISP link by tail and error...<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><br /><div class="quotebox"><blockquote><p>configure <a href="http://www.dslreports.com/speedtest">http://www.dslreports.com/speedtest</a> to use more concurrent streams (like 10/10, you probably need to try a few combination), as that should even out unhappiness of the tcp streams with the &quot;lumpy&quot; WISP link.</p></blockquote></div><p>I messed with a few types and found 10/10 to work the best. I ran the test twice ( about 5 minutes apart) and the idle MS ranged from 20 up to 90 only once. Any others I tested and it went up to 150 and beyond.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp;During the idle period no stream is supposed to be active so all idle periods should be identical; that they are not probably means there is a high variation of bandwidth on your WISP link. I had hoped that more streams might increase the reported bandwidth as more streams tends to smooth out single stream speed fluctuations but again I have never seen a WISP link so this might not work well for you.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>Annnd here is the speedguide data.</p><div class="codebox"><pre><code> « SpeedGuide.net TCP Analyzer Results » 
Tested on: 2015.06.08 19:39 
IP address: 67.43.xxx.xxx 
Client OS/browser: Windows 8.1 (Firefox 38.0) 
 
TCP options string: 020405b40103030801010402 
MSS: 1460 
MTU: 1500 
TCP Window: 65536 (NOT multiple of MSS) 
RWIN Scaling: 8 bits (2^8=256) 
Unscaled RWIN : 256 
Recommended RWINs: 64240, 128480, 256960, 513920, 1027840 
BDP limit (200ms): 2621kbps (328KBytes/s)
BDP limit (500ms): 1049kbps (131KBytes/s) 
MTU Discovery: ON 
TTL: 112 
Timestamps: OFF 
SACKs: ON 
IP ToS: 00101000 (40) 
    Precedence: 001 (priority)
    Delay: 0 (normal delay)
    Throughput: 1 (high throughput)
    Reliability: 0 (normal reliability)
    Cost: 0 (normal cost)
    Check bit: 0 (correct)
DSCP (DiffServ): AF11 001010 (10) - Assured Forwarding class 1, low drop precedence (RFC 2597). </code></pre></div><p>I am assuming that getting the data does not require me to download anything. I just connected to the site and copied the &quot;my results&quot; section.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; The data looks fine, except that the MTU is supposedly 1500, according to your ISPs web-site they use PPoE so the MTU should actually be 1492 (unless that also use baby-jumbo frames, but I digress).<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><br /><p>I noticed you went to &quot;wiser&quot; website. My ISP is Wisper. Here is a link to their site if you want to check it out. <a href="http://www.wisperisp.com/wp/">http://www.wisperisp.com/wp/</a></p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp;Oh, sorry, that is the web site I looked at but it seems my typing was a bit buggy... Now I notice that yesterday the site looked differently, and yesterdays links seem not to work anymore, but yet under <a href="http://www.wisperisp.com/wp/help-center/">http://www.wisperisp.com/wp/help-center/</a> they still mention PPPoE (which does not rhyme well with the MTU of 1500 you report above...)<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><br /><div class="quotebox"><blockquote><p>you probably should specify an overhead of 8byte for sqm-scripts</p></blockquote></div><p>lastly, in the QoS overhead settings, would I set it to &quot;ATM&quot; or &quot;Ethernet with overhead:&quot;? The openwrt wiki article doesn&#039;t specify satellite or Wisp type connections.</p></blockquote></div><p>Definitely &quot;ethernet with overhead&quot;, ATM will try to account for and ATM link layer which will reduce the effective bandwidth by ~9% from the values you plug into the SQM guy to account for ATM&#039;s 48/53 coding. Unless your link uses ATM you certainly do not want to use this setting. (ATM /AAL5has even more quirks in that it enforces that each packet is sent using an integer number of the 48 byte data cells, so worst case it can add 47 Bytes of padding after each user data packet, and this eats up additional bandwidth; also ATM links often have a high per packet overhead in addition to all the IP and TCP/UDP headers used.)</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280085">
				<div class="post-metadata">
					<div class="post-num">Post #14</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						15 Jun 2015, 00:25					</div>
				</div>
				<div class="post-content content">
					<p>Sorry, for the wait on the reply, I&#039;ve been busy the past week and haven&#039;t had a chance to do anything.</p><p>the SQM still seems to be throttling more with the link layer adaptation in place. When I put the over head to 8 and applied it, my download went down to 7 mbs. As soon as I turned the link layer adaptation off, the I started getting the &quot;normal&quot; 9 =10 mbs I get when I have the SQM activated.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280087">
				<div class="post-metadata">
					<div class="post-num">Post #15</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						15 Jun 2015, 00:35					</div>
				</div>
				<div class="post-content content">
					<p>Hi Panny P.</p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>Sorry, for the wait on the reply, I&#039;ve been busy the past week and haven&#039;t had a chance to do anything.</p><p>the SQM still seems to be throttling more with the link layer adaptation in place. When I put the over head to 8 and applied it, my download went down to 7 mbs. As soon as I turned the link layer adaptation off, the I started getting the &quot;normal&quot; 9 =10 mbs I get when I have the SQM activated.</p></blockquote></div><p>This is rather weird. Could you post the result from &quot;cat /etc/config/sqm&quot; again, once with overhead of 8 and once without, please whenever you get around to it.</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280221">
				<div class="post-metadata">
					<div class="post-num">Post #16</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						16 Jun 2015, 01:47					</div>
				</div>
				<div class="post-content content">
					<p>I did some more speedtests to see if it hadn&#039;t changed. Here they are if you&#039;re curious.</p><p>QoS off <br /> <a href="http://www.dslreports.com/speedtest/680748">http://www.dslreports.com/speedtest/680748</a></p><p>QoS on with no overhead.<br /> <a href="http://www.dslreports.com/speedtest/680762">http://www.dslreports.com/speedtest/680762</a><br /> <a href="http://www.dslreports.com/speedtest/680799">http://www.dslreports.com/speedtest/680799</a></p><p>QoS on with the 8 packet overhead.<br /> <a href="http://www.dslreports.com/speedtest/680767">http://www.dslreports.com/speedtest/680767</a><br /> <a href="http://www.dslreports.com/speedtest/680805">http://www.dslreports.com/speedtest/680805</a> - this one did finish with 9.25, but it hovered in the 8&#039;s for awhile during the download part. The ones with no overhead stayed above 8mbs down.</p><br /><p>and here is the cat results thing.</p><p>With the overhead.</p><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option qdisc_advanced &#039;0&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option enabled &#039;1&#039; </code></pre></div><p>without overhead, QoS still on.</p><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option qdisc_advanced &#039;0&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option enabled &#039;1&#039;
        option linklayer &#039;none&#039; </code></pre></div>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280282">
				<div class="post-metadata">
					<div class="post-num">Post #17</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						16 Jun 2015, 16:55					</div>
				</div>
				<div class="post-content content">
					<p>Hi Panny P.</p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>I did some more speedtests to see if it hadn&#039;t changed. Here they are if you&#039;re curious.</p><p>QoS off <br /> <a href="http://www.dslreports.com/speedtest/680748">http://www.dslreports.com/speedtest/680748</a></p><p>QoS on with no overhead.<br /> <a href="http://www.dslreports.com/speedtest/680762">http://www.dslreports.com/speedtest/680762</a><br /> <a href="http://www.dslreports.com/speedtest/680799">http://www.dslreports.com/speedtest/680799</a></p><p>QoS on with the 8 packet overhead.<br /> <a href="http://www.dslreports.com/speedtest/680767">http://www.dslreports.com/speedtest/680767</a><br /> <a href="http://www.dslreports.com/speedtest/680805">http://www.dslreports.com/speedtest/680805</a> - this one did finish with 9.25, but it hovered in the 8&#039;s for awhile during the download part. The ones with no overhead stayed above 8mbs down.</p><p>I notice that there is a difference between the QoS off speedtest and the others; in &quot;QoS off&quot; you used 3 streams for the upload in the others only two. Please do not do this, the dslreports uplink speedtest seems unfortunately a bit sensitive to the numbers of streams used, so just to be on the save side it would be great if you could reproduce the QoS off test with only two streams or better the other tests also with 3 upload streams (or redo all with say 6 streams, if that works). This might not be your issue, but I would recommend to test that nevertheless.</p><br /><p>and here is the cat results thing.</p><p>With the overhead.</p><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option qdisc_advanced &#039;0&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option enabled &#039;1&#039; </code></pre></div><p>without overhead, QoS still on.</p><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option qdisc_advanced &#039;0&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option enabled &#039;1&#039;
        option linklayer &#039;none&#039; </code></pre></div></blockquote></div><p>This is weird 10-20% Bandwidth loss for an accounting change of 8 bytes per packet; plus the maximum latency increases are really not well controlled even for the QoS on tests. It seems sqm does not work well with such a variable link as yours. One thing you could try is to increase the interval to 150ms (from the default 100ms) to account for your idle access latency which might give a little more bandwidth without affecting latency vey much (not that sqm does a good enough job controlling your latency to begin with). If you want to try that add &quot;target 10ms interval 150ms &quot; to both &quot;Advanced option string&quot; fields in the &quot;Queue Discipline&quot; tab of the SQM GUI, note you will need to check both &quot;Show and Use Advanced Configuration&quot; as well as &quot;Show and Use Dangerous Configuration&quot; to actually see these fields. Also you need to keep these two boxes checked others the exposed fields are not used...<br />&nbsp; &nbsp; &nbsp; &nbsp;If you do this, please also post the output of &quot;tc -d qdisc ; tc -s -d class show dev eth0.2 ; tc -s -d class show dev ifb4eth0.2&quot; after putting the change into the GUI and pressing the &quot;Save &amp; Apply&quot; button.<br />&nbsp; &nbsp; &nbsp; &nbsp;BTW I wonder why your configs are headed by &quot;config queue &#039;eth1&#039;&quot; in my case there is no queue name (but I am still on cerowrt so that might explain this) or did you change that manually?</p><br /><br /><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280752">
				<div class="post-metadata">
					<div class="post-num">Post #18</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						20 Jun 2015, 01:35					</div>
				</div>
				<div class="post-content content">
					<p>Here are the results after changing the settings with QoS on.</p><p>tc -d qdisc</p><div class="codebox"><pre><code> qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc mq 0: dev wlan0 root
qdisc mq 0: dev wlan1 root
qdisc mq 0: dev wlan1-1 root
qdisc mq 0: dev wlan1-2 root
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms ecn </code></pre></div><p>tc -s -d class show dev eth0.2</p><div class="codebox"><pre><code> class htb 1:11 parent 1:1 leaf 110: prio 1 quantum 1500 rate 128Kbit ceil 716Kbit linklayer ethernet burst 1600b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 238451 bytes 1972 pkt (dropped 0, overlimits 0 requeues 0)
 rate 2Kbit 2pps backlog 0b 0p requeues 0
 lended: 1942 borrowed: 30 giants: 0
 tokens: 1466796 ctokens: 262219

class htb 1:1 root rate 2150Kbit ceil 2150Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 7
 Sent 3741519 bytes 20200 pkt (dropped 0, overlimits 0 requeues 0)
 rate 5920bit 5pps backlog 0b 0p requeues 0
 lended: 2625 borrowed: 0 giants: 0
 tokens: 89410 ctokens: 89410

class htb 1:10 parent 1:1 prio 0 quantum 1500 rate 2150Kbit ceil 2150Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
 rate 0bit 0pps backlog 0b 0p requeues 0
 lended: 0 borrowed: 0 giants: 0
 tokens: 93015 ctokens: 93015

class htb 1:13 parent 1:1 leaf 130: prio 3 quantum 1500 rate 358Kbit ceil 2134Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
 rate 0bit 0pps backlog 0b 0p requeues 0
 lended: 0 borrowed: 0 giants: 0
 tokens: 558656 ctokens: 93718

class htb 1:12 parent 1:1 leaf 120: prio 2 quantum 1500 rate 358Kbit ceil 2134Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 3503068 bytes 18228 pkt (dropped 0, overlimits 0 requeues 0)
 rate 3912bit 4pps backlog 0b 0p requeues 0
 lended: 15633 borrowed: 2595 giants: 0
 tokens: 520375 ctokens: 90086

class fq_codel 110:5d parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 121 count 0 lastcount 0 ldelay 8us
class fq_codel 110:1b1 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit -335 count 0 lastcount 0 ldelay 11us
class fq_codel 110:222 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 121 count 0 lastcount 0 ldelay 8us
class fq_codel 110:22a parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 119 count 0 lastcount 0 ldelay 8us
class fq_codel 110:2db parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 202 count 0 lastcount 0 ldelay 7us
class fq_codel 110:36a parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 202 count 0 lastcount 0 ldelay 8us
class fq_codel 110:37e parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 116 count 0 lastcount 0 ldelay 8us
class fq_codel 110:3c3 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 121 count 0 lastcount 0 ldelay 9us
class fq_codel 120:67 parent 120:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 29 count 0 lastcount 0 ldelay 7us
class fq_codel 120:c3 parent 120:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 114 count 0 lastcount 0 ldelay 7us
class fq_codel 120:252 parent 120:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 166 count 0 lastcount 0 ldelay 9us</code></pre></div><p>tc -s -d class show dev ifb4eth0.2</p><div class="codebox"><pre><code> class htb 1:10 parent 1:1 leaf 110: prio 0 quantum 1500 rate 12400Kbit ceil 12400Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 13675303 bytes 20743 pkt (dropped 0, overlimits 0 requeues 0)
 rate 3032bit 3pps backlog 0b 0p requeues 0
 lended: 20743 borrowed: 0 giants: 0
 tokens: 15399 ctokens: 15399

class htb 1:1 root rate 12400Kbit ceil 12400Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 7
 Sent 13675303 bytes 20743 pkt (dropped 0, overlimits 0 requeues 0)
 rate 3032bit 3pps backlog 0b 0p requeues 0
 lended: 0 borrowed: 0 giants: 0
 tokens: 15399 ctokens: 15399

class fq_codel 110:21 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 140 count 0 lastcount 0 ldelay 11us
class fq_codel 110:10f parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 240 count 0 lastcount 0 ldelay 13us
class fq_codel 110:1b0 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 145 count 0 lastcount 0 ldelay 9us
class fq_codel 110:33f parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 156 count 0 lastcount 0 ldelay 11us
class fq_codel 110:39b parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 226 count 0 lastcount 0 ldelay 10us</code></pre></div><div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p> BTW I wonder why your configs are headed by &quot;config queue &#039;eth1&#039;&quot; in my case there is no queue name (but I am still on cerowrt so that might explain this) or did you change that manually?</p></blockquote></div><p>I don&#039;t recall changing anything. All of my google searches has led me to linux, but never Windows, which is what I use.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280756">
				<div class="post-metadata">
					<div class="post-num">Post #19</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						20 Jun 2015, 02:12					</div>
				</div>
				<div class="post-content content">
					<p>Hi Panny P.<br />thanks for the data. It looks like fq_codel accepted your target and interval changes, good. Now I guess you need to run a few speed tests to see whether this changed anything. Have a look at <a href="https://github.com/richb-hanover/OpenWrtScripts">https://github.com/richb-hanover/OpenWrtScripts</a> Rich&#039;s betterspeedtest.sh script is pretty good for link debugging as it is easy to run this for say a minute in each direction, or both directions (I think the bidirectional script is netperf-runner). I typically use the following as a quick test:<br />/betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 ; ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4<br />If you manage to install a recent enough version of netperf on your router you should be able to run this from the router, otherwise you need a unix machine or VM... </p><br /><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>Here are the results after changing the settings with QoS on.</p><p>tc -d qdisc</p><div class="codebox"><pre><code> qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc mq 0: dev wlan0 root
qdisc mq 0: dev wlan1 root
qdisc mq 0: dev wlan1-1 root
qdisc mq 0: dev wlan1-2 root
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 10.0ms interval 150.0ms ecn </code></pre></div><p>tc -s -d class show dev eth0.2</p><div class="codebox"><pre><code> class htb 1:11 parent 1:1 leaf 110: prio 1 quantum 1500 rate 128Kbit ceil 716Kbit linklayer ethernet burst 1600b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 238451 bytes 1972 pkt (dropped 0, overlimits 0 requeues 0)
 rate 2Kbit 2pps backlog 0b 0p requeues 0
 lended: 1942 borrowed: 30 giants: 0
 tokens: 1466796 ctokens: 262219

class htb 1:1 root rate 2150Kbit ceil 2150Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 7
 Sent 3741519 bytes 20200 pkt (dropped 0, overlimits 0 requeues 0)
 rate 5920bit 5pps backlog 0b 0p requeues 0
 lended: 2625 borrowed: 0 giants: 0
 tokens: 89410 ctokens: 89410

class htb 1:10 parent 1:1 prio 0 quantum 1500 rate 2150Kbit ceil 2150Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
 rate 0bit 0pps backlog 0b 0p requeues 0
 lended: 0 borrowed: 0 giants: 0
 tokens: 93015 ctokens: 93015

class htb 1:13 parent 1:1 leaf 130: prio 3 quantum 1500 rate 358Kbit ceil 2134Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
 rate 0bit 0pps backlog 0b 0p requeues 0
 lended: 0 borrowed: 0 giants: 0
 tokens: 558656 ctokens: 93718

class htb 1:12 parent 1:1 leaf 120: prio 2 quantum 1500 rate 358Kbit ceil 2134Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 3503068 bytes 18228 pkt (dropped 0, overlimits 0 requeues 0)
 rate 3912bit 4pps backlog 0b 0p requeues 0
 lended: 15633 borrowed: 2595 giants: 0
 tokens: 520375 ctokens: 90086

class fq_codel 110:5d parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 121 count 0 lastcount 0 ldelay 8us
class fq_codel 110:1b1 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit -335 count 0 lastcount 0 ldelay 11us
class fq_codel 110:222 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 121 count 0 lastcount 0 ldelay 8us
class fq_codel 110:22a parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 119 count 0 lastcount 0 ldelay 8us
class fq_codel 110:2db parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 202 count 0 lastcount 0 ldelay 7us
class fq_codel 110:36a parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 202 count 0 lastcount 0 ldelay 8us
class fq_codel 110:37e parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 116 count 0 lastcount 0 ldelay 8us
class fq_codel 110:3c3 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 121 count 0 lastcount 0 ldelay 9us
class fq_codel 120:67 parent 120:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 29 count 0 lastcount 0 ldelay 7us
class fq_codel 120:c3 parent 120:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 114 count 0 lastcount 0 ldelay 7us
class fq_codel 120:252 parent 120:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 166 count 0 lastcount 0 ldelay 9us</code></pre></div><p>tc -s -d class show dev ifb4eth0.2</p><div class="codebox"><pre><code> class htb 1:10 parent 1:1 leaf 110: prio 0 quantum 1500 rate 12400Kbit ceil 12400Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 0
 Sent 13675303 bytes 20743 pkt (dropped 0, overlimits 0 requeues 0)
 rate 3032bit 3pps backlog 0b 0p requeues 0
 lended: 20743 borrowed: 0 giants: 0
 tokens: 15399 ctokens: 15399

class htb 1:1 root rate 12400Kbit ceil 12400Kbit linklayer ethernet burst 1599b/1 mpu 0b overhead 0b cburst 1599b/1 mpu 0b overhead 0b level 7
 Sent 13675303 bytes 20743 pkt (dropped 0, overlimits 0 requeues 0)
 rate 3032bit 3pps backlog 0b 0p requeues 0
 lended: 0 borrowed: 0 giants: 0
 tokens: 15399 ctokens: 15399

class fq_codel 110:21 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 140 count 0 lastcount 0 ldelay 11us
class fq_codel 110:10f parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 240 count 0 lastcount 0 ldelay 13us
class fq_codel 110:1b0 parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 145 count 0 lastcount 0 ldelay 9us
class fq_codel 110:33f parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 156 count 0 lastcount 0 ldelay 11us
class fq_codel 110:39b parent 110:
 (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  deficit 226 count 0 lastcount 0 ldelay 10us</code></pre></div><div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p> BTW I wonder why your configs are headed by &quot;config queue &#039;eth1&#039;&quot; in my case there is no queue name (but I am still on cerowrt so that might explain this) or did you change that manually?</p></blockquote></div><p>I don&#039;t recall changing anything. All of my google searches has led me to linux, but never Windows, which is what I use.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp;It seems that this is a difference between openwrt and cerowrt, which I will see myself once I migrate to openwrt...</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280812">
				<div class="post-metadata">
					<div class="post-num">Post #20</div>
					<div class="post-author">richbhanover</div>
					<div class="post-datetime">
						20 Jun 2015, 17:07					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p>... Have a look at <a href="https://github.com/richb-hanover/OpenWrtScripts">https://github.com/richb-hanover/OpenWrtScripts</a> Rich&#039;s betterspeedtest.sh script is pretty good for link debugging as it is easy to run this for say a minute in each direction, or both directions (I think the bidirectional script is netperf-runner). I typically use the following as a quick test:<br />/betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 ; ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4<br />If you manage to install a recent enough version of netperf on your router you should be able to run this from the router, otherwise you need a unix machine or VM...</p></blockquote></div><p>Yes, the OpenWrtScripts works with BB. You can also </p><div class="codebox"><pre><code>opkg install netperf</code></pre></div><p> to get a good version of netperf installed, then run the betterspeedtest.sh or netperfrunner.sh directly from the OpenWrt router as moeller0 suggests.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280819">
				<div class="post-metadata">
					<div class="post-num">Post #21</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						20 Jun 2015, 18:05					</div>
				</div>
				<div class="post-content content">
					<p>Hi rrichbhannover,</p><div class="quotebox"><cite>richbhanover wrote:</cite><blockquote><div class="quotebox"><cite>moeller0 wrote:</cite><blockquote><p>... Have a look at <a href="https://github.com/richb-hanover/OpenWrtScripts">https://github.com/richb-hanover/OpenWrtScripts</a> Rich&#039;s betterspeedtest.sh script is pretty good for link debugging as it is easy to run this for say a minute in each direction, or both directions (I think the bidirectional script is netperf-runner). I typically use the following as a quick test:<br />/betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 ; ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4<br />If you manage to install a recent enough version of netperf on your router you should be able to run this from the router, otherwise you need a unix machine or VM...</p></blockquote></div><p>Yes, the OpenWrtScripts works with BB. You can also </p><div class="codebox"><pre><code>opkg install netperf</code></pre></div><p> to get a good version of netperf installed, then run the betterspeedtest.sh or netperfrunner.sh directly from the OpenWrt router as moeller0 suggests.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp;Rich, thanks a lot for making your nifty test scripts available for us. This is really great news, that the BB netperf is recent enough for betterspeedtest! Do you know whether it is also new enough for netperf-wrapper/flent (these require a binary compiled with the &quot;demo mode&quot;)? The beauty of running the tests directly from the router is that it will help isolate the issue and validate the theory that it is caused by sqm-scripts somehow.</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280921">
				<div class="post-metadata">
					<div class="post-num">Post #22</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						21 Jun 2015, 21:14					</div>
				</div>
				<div class="post-content content">
					<p>Okay, it took me a while to figure out what to do, but I think I got it.</p><p>Just to make sure I did it correctly and got the correct results; this is what I did:</p><p>1. Logged into putty like before.</p><p>2. entered this from github&nbsp; &quot;openwrtScripts&quot;<br />-opkg update<br />-opkg install git<br />-opkg install netperf<br />-cd /usr/lib<br />-git clone git://github.com/richb-hanover/OpenWrtScripts.git<br />3. scrolled down to betterspeedtest.sh and noticed the example, so I entered &quot;cd /usr/lib/OpenWrtScripts&quot;, then I entered the commands Moeller0 provided.</p><p>4. Then typed in &quot; ./betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 (I added the &#039;.&#039; at the beginning, not sure if it was supposed to be there or not.)&nbsp; &quot;&nbsp; | &quot; ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 &quot;</p><p>So this is what I got with SQM OFF.</p><p>./betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4</p><div class="codebox"><pre><code>  Download:  14.31 Mbps
  Latency: (in msec, 153 pings, 0.00% packet loss)
      Min: 162.043
    10pct: 301.241
   Median: 837.248
      Avg: 754.941
    90pct: 1022.568
      Max: 1099.333
..........................................................................................................................................................
   Upload:  2.86 Mbps
  Latency: (in msec, 155 pings, 0.00% packet loss)
      Min: 181.698
    10pct: 742.543
   Median: 2583.310
      Avg: 2668.689
    90pct: 4293.876
      Max: 5288.024</code></pre></div><p>this is from the ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4&nbsp; &nbsp; Test</p><div class="codebox"><pre><code> Download:  8.14 Mbps
   Upload:  2.07 Mbps
  Latency: (in msec, 156 pings, 0.00% packet loss)
      Min: 158.474
    10pct: 193.538
   Median: 855.114
      Avg: 1326.443
    90pct: 2982.612
      Max: 4171.047
  </code></pre></div><p>I ran this one 3 times since i was getting erratic results, first one download was 11.5, then 8.28, now it is 4. <br />-------------------<br />Now SQM on. </p><p>./betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4</p><div class="codebox"><pre><code> Download:  9.62 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 156.539
    10pct: 159.609
   Median: 168.368
      Avg: 176.121
    90pct: 200.509
      Max: 269.896
.......................................................................................................................................................
   Upload:  1.84 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 155.052
    10pct: 160.672
   Median: 166.160
      Avg: 169.776
    90pct: 183.261
      Max: 230.754
  </code></pre></div><p>./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 </p><div class="codebox"><pre><code> Download:  8.28 Mbps
   Upload:  1.33 Mbps
  Latency: (in msec, 154 pings, 0.00% packet loss)
      Min: 156.864
    10pct: 164.699
   Median: 187.177
      Avg: 191.871
    90pct: 220.835
      Max: 324.423
 </code></pre></div>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p280932">
				<div class="post-metadata">
					<div class="post-num">Post #23</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						21 Jun 2015, 23:01					</div>
				</div>
				<div class="post-content content">
					<p>Hello Panny P.</p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>Okay, it took me a while to figure out what to do, but I think I got it.</p><p>Just to make sure I did it correctly and got the correct results; this is what I did:</p><p>1. Logged into putty like before.</p><p>2. entered this from github&nbsp; &quot;openwrtScripts&quot;<br />-opkg update<br />-opkg install git<br />-opkg install netperf<br />-cd /usr/lib<br />-git clone git://github.com/richb-hanover/OpenWrtScripts.git<br />3. scrolled down to betterspeedtest.sh and noticed the example, so I entered &quot;cd /usr/lib/OpenWrtScripts&quot;, then I entered the commands Moeller0 provided.</p><p>4. Then typed in &quot; ./betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 (I added the &#039;.&#039; at the beginning, not sure if it was supposed to be there or not.)&nbsp; &quot;&nbsp; | &quot; ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 &quot;</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Excellent. Thanks for filling in the missing point; &quot;./&quot; denotes the present working directory, and it was required &quot;/&quot; alone would have looked for betterspeedtest.sh at the root of your file system, and that would not have worked...<br />&nbsp; &nbsp; &nbsp; &nbsp; Looking at the command line I gave you, I realize that the european netperf servers (in Sweden) are most likely not the best match for you, so you might want to try either netperf-west or netperf-east, just replace the netperf-eu in the above invocation with the other server name (so keep the .bufferbloat.net part). <br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><br /><p>So this is what I got with SQM OFF.</p><p>./betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4</p><div class="codebox"><pre><code>  Download:  14.31 Mbps
  Latency: (in msec, 153 pings, 0.00% packet loss)
      Min: 162.043
    10pct: 301.241
   Median: 837.248
      Avg: 754.941
    90pct: 1022.568
      Max: 1099.333
..........................................................................................................................................................
   Upload:  2.86 Mbps
  Latency: (in msec, 155 pings, 0.00% packet loss)
      Min: 181.698
    10pct: 742.543
   Median: 2583.310
      Avg: 2668.689
    90pct: 4293.876
      Max: 5288.024</code></pre></div></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; So the latency looks truly atrocious, a median of &gt;800ms for downlink saturation and 2.5 SECONDS for uplink saturation are truly signed of a link that is running well above if what it should. Given that netperf-eu seems roughly 160ms away an RTT under load of around 200ms would be decent and up to 300ms maybe acceptable (and to assess the effective latency under load experience I recommend to look at the 90% or Max, as the median is a bit deceiving a few delayed packets can really foul up interactive network traffic), but this is in the painful territory.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><br /><br /><p>this is from the ./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4&nbsp; &nbsp; Test</p><div class="codebox"><pre><code> Download:  8.14 Mbps
   Upload:  2.07 Mbps
  Latency: (in msec, 156 pings, 0.00% packet loss)
      Min: 158.474
    10pct: 193.538
   Median: 855.114
      Avg: 1326.443
    90pct: 2982.612
      Max: 4171.047
  </code></pre></div><p>I ran this one 3 times since i was getting erratic results, first one download was 11.5, then 8.28, now it is 4.</p></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Saturating both directions simultaneously, really shows how rough your wisp link actually behaves, median RTT &gt;800ms, 90% close to 3 seconds... and this also shows how badly the bandwidth is affected in download and upload direction. One thing to note is that when running such tests it might be worthwhile to let the link idle for a minute or two between runs, to be sure all buffers are empty again. But download speeds changing between 11.5 and 4 Mbps is a good hint that SQM off is not a good idea on this link.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>-------------------<br />Now SQM on. </p><p>./betterspeedtest.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4</p><div class="codebox"><pre><code> Download:  9.62 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 156.539
    10pct: 159.609
   Median: 168.368
      Avg: 176.121
    90pct: 200.509
      Max: 269.896
.......................................................................................................................................................
   Upload:  1.84 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 155.052
    10pct: 160.672
   Median: 166.160
      Avg: 169.776
    90pct: 183.261
      Max: 230.754
  </code></pre></div></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Latency stays pretty decent in these test, the 90% values still fall into my decent territory and even the Max numbers are not too painful.<br /></p><div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>./netperfrunner.sh -4 -H netperf-eu.bufferbloat.net -t 150 -p netperf-eu.bufferbloat.net -n 4 </p><div class="codebox"><pre><code> Download:  8.28 Mbps
   Upload:  1.33 Mbps
  Latency: (in msec, 154 pings, 0.00% packet loss)
      Min: 156.864
    10pct: 164.699
   Median: 187.177
      Avg: 191.871
    90pct: 220.835
      Max: 324.423
 </code></pre></div></blockquote></div><p>&nbsp; &nbsp; &nbsp; &nbsp; Here the 90% RTT numbers suffer a bit more and you lose bandwidth, but I assume the link stays somewhat useable.<br />&nbsp; &nbsp; &nbsp; &nbsp; One other test to try, now that you have these nice tools set up, would be to run the netperfrunner script with different interval and target settings in the shaper. For netperf-eu I would probably try interval 200ms and target 20ms (target should be in the range of 5 to 10% of interval, with smaller numbers sacrificing slightly more bandwidth but keep latency under load better in check). If you have time you might want to try:<br />interval 100ms target 5ms<br />interval 100ms target 10ms&nbsp; &nbsp; &nbsp; &nbsp; <br />interval 100ms target 20ms</p><p>interval 200ms target 10ms&nbsp; &nbsp; &nbsp; &nbsp; <br />interval 200ms target 20ms<br />interval 200ms target 40ms</p><p>This might give an idea whether playing with target and interval has a chance of working. But by all means also try against netperf-east and netperf-west (being closer these should have RTTs closer to what you typically see, and shorter RTTs usually means the control loop is tighter which might allow for a better bandwidth conservation). If you do these tests, please ask post the results of &quot;cat /etc/config/sqm ; tc -d qdisc&quot; for each of the settings. And also please note whether the tests were performed during the congestion hours of the link of in the idler periods (I seem to recall that you tested during the crunch hours so far, but how does the data look in the idler times?). Thanks in advance</p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p281806">
				<div class="post-metadata">
					<div class="post-num">Post #24</div>
					<div class="post-author">Panny P</div>
					<div class="post-datetime">
						29 Jun 2015, 21:57					</div>
				</div>
				<div class="post-content content">
					<p>Sorry for the long reply, had a hectic week. And the internet connection seems to be getting worse during peak hours.</p><p>I make sure that my speeds are fine before I do any testing. I can&#039;t vouch if it coincidentally tanks after I check though.</p><p>Here are all the tests. I accessed east coast servers, but I live in the middle of the U.S. So pings may still be higher than normal. Most of the servers I use are based on East Coast anyway.</p><p><strong>Target 5ms Interval 100ms</strong></p><p>cat /etc/config/sqm</p><div class="codebox"><pre><code>config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option linklayer_advanced &#039;1&#039;
        option tcMTU &#039;2047&#039;
        option tcTSIZE &#039;128&#039;
        option tcMPU &#039;0&#039;
        option linklayer_adaptation_mechanism &#039;tc_stab&#039;
        option enabled &#039;1&#039;
        option iqdisc_opts &#039;target 5ms interval 100ms&#039;
        option eqdisc_opts &#039;target 5ms interval 100ms&#039; </code></pre></div><p>tc -d qdisc</p><div class="codebox"><pre><code> qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 5.0ms interval 100.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 5.0ms interval 100.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 5.0ms interval 100.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn</code></pre></div><br /><div class="codebox"><pre><code>  Download:  9.83 Mbps
   Upload:  1.62 Mbps
  Latency: (in msec, 144 pings, 0.00% packet loss)
      Min: 38.216
    10pct: 42.876
   Median: 50.279
      Avg: 52.218
    90pct: 63.743
      Max: 103.990</code></pre></div><br /><br /><div class="codebox"><pre><code> Download:  6.54 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 37.366
    10pct: 45.474
   Median: 62.732
      Avg: 71.595
    90pct: 101.756
      Max: 200.042
.......................................................................................................................................................
   Upload:  1.8 Mbps
  Latency: (in msec, 148 pings, 0.00% packet loss)
      Min: 35.958
    10pct: 39.426
   Median: 46.397
      Avg: 51.981
    90pct: 71.966
      Max: 157.282</code></pre></div><p><strong>Target 10 Interval 100 </strong></p><p>cat /etc/config/sqm</p><div class="codebox"><pre><code>config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option linklayer_advanced &#039;1&#039;
        option tcMTU &#039;2047&#039;
        option tcTSIZE &#039;128&#039;
        option tcMPU &#039;0&#039;
        option linklayer_adaptation_mechanism &#039;tc_stab&#039;
        option enabled &#039;1&#039;
        option iqdisc_opts &#039;target 10ms interval 100ms&#039;
        option eqdisc_opts &#039;target 10ms interval 100ms&#039;</code></pre></div><p>tc -d qdisc</p><div class="codebox"><pre><code>qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 10.0ms interval 100.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 10.0ms interval 100.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 10.0ms interval 100.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 10.0ms interval 100.0ms ecn</code></pre></div><br /><br /><div class="codebox"><pre><code> Download:  10.38 Mbps
   Upload:  1.6 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 37.663
    10pct: 40.609
   Median: 47.723
      Avg: 49.354
    90pct: 57.691
      Max: 131.406</code></pre></div><br /><br /><div class="codebox"><pre><code>  Download:  10.68 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 35.358
    10pct: 38.664
   Median: 43.321
      Avg: 46.755
    90pct: 54.223
      Max: 164.652
......................................................................................................................................................
   Upload:  1.89 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 36.024
    10pct: 39.621
   Median: 46.924
      Avg: 49.498
    90pct: 57.677
      Max: 176.323</code></pre></div><p><strong> Target 20 Interval 100 </strong></p><p>cat /etc/config/sqm</p><div class="codebox"><pre><code>config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option linklayer_advanced &#039;1&#039;
        option tcMTU &#039;2047&#039;
        option tcTSIZE &#039;128&#039;
        option tcMPU &#039;0&#039;
        option linklayer_adaptation_mechanism &#039;tc_stab&#039;
        option enabled &#039;1&#039;
        option iqdisc_opts &#039;target 20ms interval 100ms&#039;
        option eqdisc_opts &#039;target 20ms interval 100ms&#039;</code></pre></div><p>tc -d qdisc</p><div class="codebox"><pre><code> 
qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 20.0ms interval 100.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 20.0ms interval 100.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 20.0ms interval 100.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 20.0ms interval 100.0ms ecn</code></pre></div><br /><div class="codebox"><pre><code>  Download:  8.49 Mbps
   Upload:  1.23 Mbps
  Latency: (in msec, 156 pings, 0.00% packet loss)
      Min: 39.501
    10pct: 46.538
   Median: 65.434
      Avg: 72.059
    90pct: 105.123
      Max: 191.904</code></pre></div><br /><div class="codebox"><pre><code> Download:  9.64 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 37.743
    10pct: 44.953
   Median: 59.494
      Avg: 64.588
    90pct: 85.698
      Max: 200.167
......................................................................................................................................................
   Upload:  1.93 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 37.077
    10pct: 40.687
   Median: 47.337
      Avg: 49.165
    90pct: 58.685
      Max: 80.746</code></pre></div><p> </p><p><strong> target 10ms interval 200ms </strong></p><br /><p>cat /etc/config/sqm</p><div class="codebox"><pre><code>config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option linklayer_advanced &#039;1&#039;
        option tcMTU &#039;2047&#039;
        option tcTSIZE &#039;128&#039;
        option tcMPU &#039;0&#039;
        option linklayer_adaptation_mechanism &#039;tc_stab&#039;
        option iqdisc_opts &#039;target 10ms interval 200ms&#039;
        option eqdisc_opts &#039;target 10ms interval 200ms&#039;
        option enabled &#039;1&#039;</code></pre></div><p>tc -d qdisc</p><div class="codebox"><pre><code>qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 1 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 10.0ms interval 200.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 10.0ms interval 200.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 10.0ms interval 200.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 10.0ms interval 200.0ms ecn</code></pre></div><br /><div class="codebox"><pre><code>   Download:  8.69 Mbps
   Upload:  1.21 Mbps
  Latency: (in msec, 153 pings, 0.00% packet loss)
      Min: 40.383
    10pct: 49.969
   Median: 74.272
      Avg: 81.072
    90pct: 122.590
      Max: 195.737</code></pre></div><br /><div class="codebox"><pre><code> 
 Download:  9.73 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 37.857
    10pct: 48.224
   Median: 80.058
      Avg: 82.704
    90pct: 121.139
      Max: 168.444
.......................................................................................................................................................
   Upload:  1.72 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 37.248
    10pct: 47.920
   Median: 65.504
      Avg: 70.106
    90pct: 96.518
      Max: 253.972

  </code></pre></div><p><strong> target 20ms interval 200ms </strong></p><div class="codebox"><pre><code> config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option linklayer_advanced &#039;1&#039;
        option tcMTU &#039;2047&#039;
        option tcTSIZE &#039;128&#039;
        option tcMPU &#039;0&#039;
        option linklayer_adaptation_mechanism &#039;tc_stab&#039;
        option enabled &#039;1&#039;
        option iqdisc_opts &#039;target 20ms interval 200ms&#039;
        option eqdisc_opts &#039;target 20ms interval 200ms&#039;</code></pre></div><p>tc -d qdisc</p><div class="codebox"><pre><code> 
qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 20.0ms interval 200.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 20.0ms interval 200.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 20.0ms interval 200.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 20.0ms interval 200.0ms ecn</code></pre></div><br /><br /><div class="codebox"><pre><code> Download:  10.13 Mbps
   Upload:  1.43 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 36.757
    10pct: 45.327
   Median: 59.954
      Avg: 65.206
    90pct: 90.410
      Max: 142.188</code></pre></div><br /><div class="codebox"><pre><code>  Download:  9.77 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 40.124
    10pct: 49.679
   Median: 80.165
      Avg: 85.261
    90pct: 121.251
      Max: 259.709
.......................................................................................................................................................
   Upload:  1.68 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 37.264
    10pct: 47.377
   Median: 67.795
      Avg: 72.486
    90pct: 103.882
      Max: 137.415</code></pre></div><p><strong> Target 40 Interval 200 </strong></p><p>cat /etc/config/sqm</p><div class="codebox"><pre><code> 
config queue &#039;eth1&#039;
        option qdisc &#039;fq_codel&#039;
        option script &#039;simple.qos&#039;
        option interface &#039;eth0.2&#039;
        option upload &#039;2150&#039;
        option download &#039;12400&#039;
        option linklayer &#039;ethernet&#039;
        option overhead &#039;8&#039;
        option qdisc_advanced &#039;1&#039;
        option squash_dscp &#039;1&#039;
        option squash_ingress &#039;1&#039;
        option ingress_ecn &#039;ECN&#039;
        option egress_ecn &#039;NOECN&#039;
        option qdisc_really_really_advanced &#039;1&#039;
        option linklayer_advanced &#039;1&#039;
        option tcMTU &#039;2047&#039;
        option tcTSIZE &#039;128&#039;
        option tcMPU &#039;0&#039;
        option linklayer_adaptation_mechanism &#039;tc_stab&#039;
        option enabled &#039;1&#039;
        option iqdisc_opts &#039;target 40ms interval 200ms&#039;
        option eqdisc_opts &#039;target 40ms interval 200ms&#039;</code></pre></div><p>tc -d qdisc</p><div class="codebox"><pre><code> qdisc fq_codel 0: dev eth0 root refcnt 2 limit 1024p flows 1024 quantum 300 target 5.0ms interval 100.0ms ecn
qdisc htb 1: dev eth0.2 root refcnt 2 r2q 10 default 12 direct_packets_stat 0 ver 3.17 direct_qlen 2
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev eth0.2 parent 1:11 limit 1001p flows 1024 quantum 300 target 40.0ms interval 200.0ms
qdisc fq_codel 120: dev eth0.2 parent 1:12 limit 1001p flows 1024 quantum 300 target 40.0ms interval 200.0ms
qdisc fq_codel 130: dev eth0.2 parent 1:13 limit 1001p flows 1024 quantum 300 target 40.0ms interval 200.0ms
qdisc ingress ffff: dev eth0.2 parent ffff:fff1 ----------------
qdisc htb 1: dev ifb4eth0.2 root refcnt 2 r2q 10 default 10 direct_packets_stat 0 ver 3.17 direct_qlen 32
 linklayer ethernet overhead 8
qdisc fq_codel 110: dev ifb4eth0.2 parent 1:10 limit 1001p flows 1024 quantum 300 target 40.0ms interval 200.0ms ecn</code></pre></div><br /><br /><div class="codebox"><pre><code>  Download:  9.33 Mbps
   Upload:  1.09 Mbps
  Latency: (in msec, 152 pings, 0.00% packet loss)
      Min: 40.302
    10pct: 45.811
   Median: 73.494
      Avg: 80.517
    90pct: 117.673
      Max: 280.648</code></pre></div><div class="codebox"><pre><code> Download:  11.17 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 36.148
    10pct: 41.031
   Median: 52.059
      Avg: 60.290
    90pct: 83.678
      Max: 232.921
......................................................................................................................................................
   Upload:  1.91 Mbps
  Latency: (in msec, 151 pings, 0.00% packet loss)
      Min: 36.863
    10pct: 41.776
   Median: 52.710
      Avg: 58.863
    90pct: 87.092
      Max: 128.200</code></pre></div>									</div>
			</article>

			
		
			
		
		
			<article class="post" id="p281826">
				<div class="post-metadata">
					<div class="post-num">Post #25</div>
					<div class="post-author">moeller0</div>
					<div class="post-datetime">
						29 Jun 2015, 23:40					</div>
				</div>
				<div class="post-content content">
					<div class="quotebox"><cite>Panny P wrote:</cite><blockquote><p>Sorry for the long reply, had a hectic week. And the internet connection seems to be getting worse during peak hours.</p><p>I make sure that my speeds are fine before I do any testing. I can&#039;t vouch if it coincidentally tanks after I check though.</p><p>Here are all the tests. I accessed east coast servers, but I live in the middle of the U.S. So pings may still be higher than normal. Most of the servers I use are based on East Coast anyway.<br />[snipp]</p></blockquote></div><p>Okay, here are the relevant? results in a table:<br />target 5ms interval 100ms<br />target[ms]&nbsp; &nbsp; interval[ms]&nbsp; &nbsp; Down[Mbps]&nbsp; &nbsp; Up[Mbps]&nbsp; &nbsp; minRTT[ms]&nbsp; &nbsp; medianRTT[ms]&nbsp; &nbsp; 90%RTT[ms]&nbsp; &nbsp; maxRTT[ms]<br />5&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; 9.83&nbsp; &nbsp; &nbsp; &nbsp; 1.62&nbsp; &nbsp; &nbsp; &nbsp; 38.216&nbsp; &nbsp; &nbsp; &nbsp; 50.279&nbsp; &nbsp; &nbsp; &nbsp; 63.743&nbsp; &nbsp; &nbsp; &nbsp; 103.990<br />5&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; 6.54&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 37.366&nbsp; &nbsp; &nbsp; &nbsp; 62.732&nbsp; &nbsp; &nbsp; &nbsp; 101.756&nbsp; &nbsp; &nbsp; &nbsp; 200.042<br />5&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 1.8&nbsp; &nbsp; &nbsp; &nbsp; 35.958&nbsp; &nbsp; &nbsp; &nbsp; 46.397&nbsp; &nbsp; &nbsp; &nbsp; 71.966&nbsp; &nbsp; &nbsp; &nbsp; 157.282</p><p>10&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; 10.38&nbsp; &nbsp; &nbsp; &nbsp; 1.6&nbsp; &nbsp; &nbsp; &nbsp; 37.663&nbsp; &nbsp; &nbsp; &nbsp; 47.723&nbsp; &nbsp; &nbsp; &nbsp; 57.691&nbsp; &nbsp; &nbsp; &nbsp; 131.406<br />10&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; 10.68&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 35.358&nbsp; &nbsp; &nbsp; &nbsp; 43.321&nbsp; &nbsp; &nbsp; &nbsp; 54.223&nbsp; &nbsp; &nbsp; &nbsp; 164.652<br />10&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 1.89&nbsp; &nbsp; &nbsp; &nbsp; 36.024&nbsp; &nbsp; &nbsp; &nbsp; 46.924&nbsp; &nbsp; &nbsp; &nbsp; 57.677&nbsp; &nbsp; &nbsp; &nbsp; 176.323</p><p>20&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; 8.49&nbsp; &nbsp; &nbsp; &nbsp; 1.23&nbsp; &nbsp; &nbsp; &nbsp; 39.501&nbsp; &nbsp; &nbsp; &nbsp; 65.434&nbsp; &nbsp; &nbsp; &nbsp; 105.123&nbsp; &nbsp; &nbsp; &nbsp; 191.904<br />20&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; 9.64&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 37.743&nbsp; &nbsp; &nbsp; &nbsp; 64.588&nbsp; &nbsp; &nbsp; &nbsp; 85.698 &nbsp; &nbsp; &nbsp; &nbsp; 200.167<br />20&nbsp; &nbsp; &nbsp; &nbsp; 100&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 1.93&nbsp; &nbsp; &nbsp; &nbsp; 37.077&nbsp; &nbsp; &nbsp; &nbsp; 47.337&nbsp; &nbsp; &nbsp; &nbsp; 58.685&nbsp; &nbsp; &nbsp; &nbsp; 80.746</p><p>10&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; 8.69&nbsp; &nbsp; &nbsp; &nbsp; 1.21&nbsp; &nbsp; &nbsp; &nbsp; 40.383&nbsp; &nbsp; &nbsp; &nbsp; 74.272&nbsp; &nbsp; &nbsp; &nbsp; 122.590&nbsp; &nbsp; &nbsp; &nbsp; 195.737<br />10&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; 9.73&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 37.857&nbsp; &nbsp; &nbsp; &nbsp; 80.058&nbsp; &nbsp; &nbsp; &nbsp; 121.139&nbsp; &nbsp; &nbsp; &nbsp; 168.444<br />10&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 1.72&nbsp; &nbsp; &nbsp; &nbsp; 37.248&nbsp; &nbsp; &nbsp; &nbsp; 65.504&nbsp; &nbsp; &nbsp; &nbsp; 96.518&nbsp; &nbsp; &nbsp; &nbsp; 253.972</p><p>20&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; 10.13&nbsp; &nbsp; &nbsp; &nbsp; 1.43&nbsp; &nbsp; &nbsp; &nbsp; 36.757&nbsp; &nbsp; &nbsp; &nbsp; 59.954&nbsp; &nbsp; &nbsp; &nbsp; 90.410&nbsp; &nbsp; &nbsp; &nbsp; 142.188<br />20&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; 9.77&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 40.124&nbsp; &nbsp; &nbsp; &nbsp; 80.165&nbsp; &nbsp; &nbsp; &nbsp; 121.251&nbsp; &nbsp; &nbsp; &nbsp; 259.709<br />20&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 1.68&nbsp; &nbsp; &nbsp; &nbsp; 37.264&nbsp; &nbsp; &nbsp; &nbsp; 67.795&nbsp; &nbsp; &nbsp; &nbsp; 103.882&nbsp; &nbsp; &nbsp; &nbsp; 137.415</p><p>40&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; 9.33&nbsp; &nbsp; &nbsp; &nbsp; 1.09&nbsp; &nbsp; &nbsp; &nbsp; 40.302&nbsp; &nbsp; &nbsp; &nbsp; 73.494&nbsp; &nbsp; &nbsp; &nbsp; 117.673&nbsp; &nbsp; &nbsp; &nbsp; 280.648<br />40&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; 11.17&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 36.148&nbsp; &nbsp; &nbsp; &nbsp; 52.059&nbsp; &nbsp; &nbsp; &nbsp; 83.678&nbsp; &nbsp; &nbsp; &nbsp; 232.921<br />40&nbsp; &nbsp; &nbsp; &nbsp; 200&nbsp; &nbsp; &nbsp; &nbsp; -&nbsp; &nbsp; &nbsp; &nbsp; 1.91&nbsp; &nbsp; &nbsp; &nbsp; 36.863&nbsp; &nbsp; &nbsp; &nbsp; 52.710&nbsp; &nbsp; &nbsp; &nbsp; 87.092&nbsp; &nbsp; &nbsp; &nbsp; 128.200</p><p>And alas, I can not make much of a rhyme out of it. There is so much variability from run to run that I can not really see a clear picture. In theory increasing interval and/or the target/interval ratio both should result in a better bandwidth usage at a cost of a higher latency. And not even that is visible in your data. I am at a loss and out of ideas, it seems on your link the trade-off between decent latency and decent bandwidth usage is pretty skewed.</p><p>There is basically one more question I have, namely what largest non-fragmenting MTU value do you get when you follow the instruction from: <a href="http://strongvpn.com/mtu_ping_test.html">http://strongvpn.com/mtu_ping_test.html</a> ? But I would be very surprised if that information would actually help figuring out what is happening with your link.. </p><p>Best Regards<br />&nbsp; &nbsp; &nbsp; &nbsp; M.</p>									</div>
			</article>

			
		
			
		
	
	<div class="pagination"><div class="pagination-number">Page 1 of 2</div><nav><ul><li class="pagination-current"><span>1</span></li><li><a href="viewtopic.php%3Fid=57729&amp;p=2.html">2</a></li></ul></nav></div>
</main>

</div>


<!-- Created in a hurry and not indicative of usual code quality. Here's a number: 0.001 -->

</body>
</html>